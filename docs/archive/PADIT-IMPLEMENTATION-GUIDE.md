# PADIT êµ¬í˜„ ê°€ì´ë“œ

**í”„ë¡œì íŠ¸ëª…**: PADIT (Protocol Anomaly Detection & Intelligence Toolkit)
**ëª©ì **: ì‹¤ì œ êµ¬í˜„ì„ ìœ„í•œ ìƒì„¸ ê°€ì´ë“œ

---

## ğŸ¯ Phase 1: í”„ë¡œì íŠ¸ ì´ˆê¸°í™” (Week 1-2)

### 1.1 Git ì €ì¥ì†Œ ìƒì„±

```bash
# ìƒˆ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/projects/padit
cd ~/projects/padit

# Git ì´ˆê¸°í™”
git init
git branch -M main

# ê¸°ë³¸ êµ¬ì¡° ìƒì„±
mkdir -p {services,shared,training,web,plugins,config,tests,docs}
mkdir -p services/{ingestion,feature_engineering,detection,alert,api}
mkdir -p shared/{domain,events,messaging,storage}
mkdir -p training/{pipelines,experiments,models}
mkdir -p plugins/{protocol_adapters,detectors,notifiers}
mkdir -p config/{base,dev,prod}
mkdir -p tests/{unit,integration,e2e}
mkdir -p docs/{architecture,api,deployment,tutorials}

# .gitignore ìƒì„±
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
.venv/
venv/
*.egg-info/
dist/
build/

# IDE
.vscode/
.idea/
*.swp
*.swo

# Environment
.env
.env.local
*.local.yaml

# Data
data/
logs/
*.log

# Models
models/*.pth
models/*.pkl
!models/.gitkeep

# Docker
docker-compose.override.yml

# OS
.DS_Store
Thumbs.db
EOF

# README ìƒì„±
cat > README.md << 'EOF'
# PADIT - Protocol Anomaly Detection & Intelligence Toolkit

Universal protocol anomaly detection platform powered by ML/AI.

## Quick Start

```bash
# Clone
git clone https://github.com/your-org/padit.git
cd padit

# Setup
docker-compose up -d

# Access
http://localhost:8080  # API
http://localhost:3000  # Web UI
http://localhost:3100  # Grafana
```

## Features

- ğŸ”Œ **Protocol Agnostic**: Support any protocol via plugins
- ğŸ¤– **AI-Powered**: AutoML, hyperparameter tuning
- ğŸ“Š **Real-time**: Streaming detection with low latency
- ğŸ—ï¸ **Production-Ready**: Kubernetes-native, HA, auto-scaling
- ğŸ”§ **Developer-Friendly**: Easy plugin development

## Documentation

- [Architecture](docs/architecture/README.md)
- [API Reference](docs/api/README.md)
- [Plugin Development](docs/tutorials/plugin-development.md)
- [Deployment](docs/deployment/README.md)

## License

Apache 2.0
EOF

git add .
git commit -m "Initial commit: project structure"
```

### 1.2 í•µì‹¬ ë„ë©”ì¸ ëª¨ë¸ ì •ì˜

```python
# shared/domain/metric.py
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, Optional
from enum import Enum

class MetricType(Enum):
    """ë©”íŠ¸ë¦­ íƒ€ì…"""
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    ERROR_RATE = "error_rate"
    AVAILABILITY = "availability"
    CUSTOM = "custom"

@dataclass(frozen=True)
class Metric:
    """í”„ë¡œí† ì½œ ë…ë¦½ì  ë©”íŠ¸ë¦­"""

    # í•„ìˆ˜ í•„ë“œ
    timestamp: datetime
    source_id: str              # ì—”ë“œí¬ì¸íŠ¸/ë””ë°”ì´ìŠ¤ ID
    metric_name: str            # ë©”íŠ¸ë¦­ ì´ë¦„
    value: float                # ë©”íŠ¸ë¦­ ê°’

    # ì„ íƒ í•„ë“œ
    unit: Optional[str] = None  # ë‹¨ìœ„ (ms, bytes, %, ...)
    metric_type: MetricType = MetricType.CUSTOM
    protocol: Optional[str] = None  # í”„ë¡œí† ì½œ ì´ë¦„
    metadata: Dict[str, Any] = None  # ì¶”ê°€ ë©”íƒ€ë°ì´í„°

    def __post_init__(self):
        if self.metadata is None:
            object.__setattr__(self, 'metadata', {})

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "timestamp": self.timestamp.isoformat(),
            "source_id": self.source_id,
            "metric_name": self.metric_name,
            "value": self.value,
            "unit": self.unit,
            "metric_type": self.metric_type.value,
            "protocol": self.protocol,
            "metadata": self.metadata,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Metric':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        return cls(
            timestamp=datetime.fromisoformat(data["timestamp"]),
            source_id=data["source_id"],
            metric_name=data["metric_name"],
            value=float(data["value"]),
            unit=data.get("unit"),
            metric_type=MetricType(data.get("metric_type", "custom")),
            protocol=data.get("protocol"),
            metadata=data.get("metadata", {}),
        )


# shared/domain/features.py
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional
import numpy as np

@dataclass
class Features:
    """í”¼ì²˜ ë²¡í„°"""

    timestamp: datetime
    source_id: str
    window_start: datetime
    window_end: datetime

    # í†µê³„ í”¼ì²˜
    mean: float
    std: float
    min: float
    max: float
    median: float

    # ë¶„ìœ„ìˆ˜
    p25: float
    p75: float
    p95: float
    p99: float

    # ì‹œê³„ì—´ í”¼ì²˜
    trend: Optional[float] = None       # ì¶”ì„¸
    seasonality: Optional[float] = None  # ê³„ì ˆì„±
    ewma: Optional[float] = None        # ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· 

    # ë³€í™”ëŸ‰
    gradient: Optional[float] = None    # ê¸°ìš¸ê¸°
    cusum: Optional[float] = None       # CUSUM

    # ì›ë³¸ ë°ì´í„° (ì„ íƒ)
    raw_values: Optional[List[float]] = None
    metadata: Dict[str, Any] = None

    def to_array(self) -> np.ndarray:
        """NumPy ë°°ì—´ë¡œ ë³€í™˜ (ML ëª¨ë¸ ì…ë ¥ìš©)"""
        return np.array([
            self.mean, self.std, self.min, self.max, self.median,
            self.p25, self.p75, self.p95, self.p99,
            self.trend or 0.0,
            self.seasonality or 0.0,
            self.ewma or 0.0,
            self.gradient or 0.0,
            self.cusum or 0.0,
        ])


# shared/domain/detection.py
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional
from enum import Enum

class Severity(Enum):
    """ì‹¬ê°ë„"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"

@dataclass
class DetectionScore:
    """íƒì§€ ì ìˆ˜"""

    timestamp: datetime
    source_id: str

    # ê°œë³„ íƒì§€ê¸° ì ìˆ˜
    rule_score: float = 0.0
    changepoint_score: float = 0.0
    residual_score: float = 0.0
    multivariate_score: float = 0.0

    # ì¢…í•© ì ìˆ˜
    composite_score: float = 0.0

    # ì´ìƒ ì—¬ë¶€
    is_anomaly: bool = False
    confidence: float = 0.0

    # ê·¼ê±°
    evidence: Dict[str, Any] = None
    detector_name: Optional[str] = None

    def __post_init__(self):
        if self.evidence is None:
            object.__setattr__(self, 'evidence', {})

@dataclass
class Alert:
    """ì•Œë¦¼"""

    alert_id: str
    timestamp: datetime
    source_id: str

    severity: Severity
    title: str
    description: str

    # íƒì§€ ì •ë³´
    detection_score: DetectionScore

    # ìƒíƒœ
    is_acknowledged: bool = False
    is_resolved: bool = False

    # ë©”íƒ€ë°ì´í„°
    tags: List[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.tags is None:
            object.__setattr__(self, 'tags', [])
        if self.metadata is None:
            object.__setattr__(self, 'metadata', {})


# shared/domain/__init__.py
from .metric import Metric, MetricType
from .features import Features
from .detection import DetectionScore, Alert, Severity

__all__ = [
    "Metric",
    "MetricType",
    "Features",
    "DetectionScore",
    "Alert",
    "Severity",
]
```

### 1.3 ì´ë²¤íŠ¸ ì •ì˜

```python
# shared/events/base.py
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Any, Dict
import uuid
import json

@dataclass
class Event(ABC):
    """Base Event"""

    event_id: str = None
    event_type: str = None
    timestamp: datetime = None

    def __post_init__(self):
        if self.event_id is None:
            object.__setattr__(self, 'event_id', str(uuid.uuid4()))
        if self.timestamp is None:
            object.__setattr__(self, 'timestamp', datetime.utcnow())
        if self.event_type is None:
            object.__setattr__(self, 'event_type', self.__class__.__name__)

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        return data

    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return json.dumps(self.to_dict())

    @classmethod
    @abstractmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Event':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        pass

    @property
    def topic(self) -> str:
        """ì´ë²¤íŠ¸ í† í”½ (Kafka topic)"""
        return f"{self.event_type.lower().replace('_', '.')}"


# shared/events/metric_events.py
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Any
from .base import Event
from ..domain import Metric

@dataclass
class MetricCollected(Event):
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì´ë²¤íŠ¸"""

    metric: Metric = None

    def to_dict(self) -> Dict[str, Any]:
        data = super().to_dict()
        data['metric'] = self.metric.to_dict()
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'MetricCollected':
        return cls(
            event_id=data['event_id'],
            event_type=data['event_type'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            metric=Metric.from_dict(data['metric']),
        )


@dataclass
class FeaturesExtracted(Event):
    """í”¼ì²˜ ì¶”ì¶œ ì´ë²¤íŠ¸"""

    features: Dict[str, Any] = None  # Featuresë¥¼ dictë¡œ ì§ë ¬í™”

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'FeaturesExtracted':
        return cls(
            event_id=data['event_id'],
            event_type=data['event_type'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            features=data['features'],
        )


@dataclass
class AnomalyDetected(Event):
    """ì´ìƒ íƒì§€ ì´ë²¤íŠ¸"""

    detection: Dict[str, Any] = None  # DetectionScoreë¥¼ dictë¡œ ì§ë ¬í™”

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AnomalyDetected':
        return cls(
            event_id=data['event_id'],
            event_type=data['event_type'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            detection=data['detection'],
        )


@dataclass
class AlertCreated(Event):
    """ì•Œë¦¼ ìƒì„± ì´ë²¤íŠ¸"""

    alert: Dict[str, Any] = None

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AlertCreated':
        return cls(
            event_id=data['event_id'],
            event_type=data['event_type'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            alert=data['alert'],
        )
```

### 1.4 ë©”ì‹œì§€ ë²„ìŠ¤ ì¶”ìƒí™”

```python
# shared/messaging/base.py
from abc import ABC, abstractmethod
from typing import Callable, Any
from ..events.base import Event

class MessageBroker(ABC):
    """ë©”ì‹œì§€ ë¸Œë¡œì»¤ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    async def connect(self):
        """ì—°ê²°"""
        pass

    @abstractmethod
    async def disconnect(self):
        """ì—°ê²° í•´ì œ"""
        pass

    @abstractmethod
    async def publish(self, topic: str, message: str):
        """ë©”ì‹œì§€ ë°œí–‰"""
        pass

    @abstractmethod
    async def subscribe(self, topic: str, handler: Callable[[str], Any]):
        """ë©”ì‹œì§€ êµ¬ë…"""
        pass


# shared/messaging/kafka_broker.py
from aiokafka import AIOKafkaProducer, AIOKafkaConsumer
from typing import Callable, Any
import asyncio
import logging
from .base import MessageBroker

logger = logging.getLogger(__name__)

class KafkaBroker(MessageBroker):
    """Kafka ë¸Œë¡œì»¤ êµ¬í˜„"""

    def __init__(self, bootstrap_servers: str):
        self.bootstrap_servers = bootstrap_servers
        self.producer: AIOKafkaProducer = None
        self.consumers: dict = {}

    async def connect(self):
        """Kafka ì—°ê²°"""
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            value_serializer=lambda v: v.encode('utf-8')
        )
        await self.producer.start()
        logger.info(f"Kafka producer connected: {self.bootstrap_servers}")

    async def disconnect(self):
        """Kafka ì—°ê²° í•´ì œ"""
        if self.producer:
            await self.producer.stop()

        for consumer in self.consumers.values():
            await consumer.stop()

        logger.info("Kafka disconnected")

    async def publish(self, topic: str, message: str):
        """ë©”ì‹œì§€ ë°œí–‰"""
        if not self.producer:
            raise RuntimeError("Producer not connected")

        await self.producer.send_and_wait(topic, message)
        logger.debug(f"Published to {topic}: {message[:100]}...")

    async def subscribe(self, topic: str, handler: Callable[[str], Any]):
        """ë©”ì‹œì§€ êµ¬ë…"""
        consumer = AIOKafkaConsumer(
            topic,
            bootstrap_servers=self.bootstrap_servers,
            group_id=f"padit-{topic}-consumer",
            value_deserializer=lambda v: v.decode('utf-8')
        )

        await consumer.start()
        self.consumers[topic] = consumer

        logger.info(f"Subscribed to topic: {topic}")

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë©”ì‹œì§€ ì†Œë¹„
        asyncio.create_task(self._consume(consumer, handler))

    async def _consume(self, consumer: AIOKafkaConsumer, handler: Callable):
        """ë©”ì‹œì§€ ì†Œë¹„ (ë°±ê·¸ë¼ìš´ë“œ)"""
        try:
            async for message in consumer:
                try:
                    await handler(message.value)
                except Exception as e:
                    logger.error(f"Handler error: {e}")
        except Exception as e:
            logger.error(f"Consumer error: {e}")


# shared/messaging/redis_broker.py (ê°„ë‹¨í•œ ëŒ€ì•ˆ)
import aioredis
from typing import Callable, Any
import asyncio
import logging
from .base import MessageBroker

logger = logging.getLogger(__name__)

class RedisBroker(MessageBroker):
    """Redis Streams ë¸Œë¡œì»¤ êµ¬í˜„ (ê²½ëŸ‰ ëŒ€ì•ˆ)"""

    def __init__(self, redis_url: str):
        self.redis_url = redis_url
        self.redis: aioredis.Redis = None
        self.running = True

    async def connect(self):
        """Redis ì—°ê²°"""
        self.redis = await aioredis.create_redis_pool(self.redis_url)
        logger.info(f"Redis connected: {self.redis_url}")

    async def disconnect(self):
        """Redis ì—°ê²° í•´ì œ"""
        self.running = False
        if self.redis:
            self.redis.close()
            await self.redis.wait_closed()
        logger.info("Redis disconnected")

    async def publish(self, topic: str, message: str):
        """ë©”ì‹œì§€ ë°œí–‰ (Redis Streams XADD)"""
        if not self.redis:
            raise RuntimeError("Redis not connected")

        await self.redis.xadd(topic, {'data': message})
        logger.debug(f"Published to {topic}")

    async def subscribe(self, topic: str, handler: Callable[[str], Any]):
        """ë©”ì‹œì§€ êµ¬ë… (Redis Streams XREAD)"""
        if not self.redis:
            raise RuntimeError("Redis not connected")

        logger.info(f"Subscribed to topic: {topic}")

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë©”ì‹œì§€ ì†Œë¹„
        asyncio.create_task(self._consume(topic, handler))

    async def _consume(self, topic: str, handler: Callable):
        """ë©”ì‹œì§€ ì†Œë¹„ (í´ë§)"""
        last_id = '0-0'

        while self.running:
            try:
                messages = await self.redis.xread(
                    [topic],
                    latest_ids=[last_id],
                    timeout=1000
                )

                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        last_id = msg_id
                        message = fields[b'data'].decode('utf-8')

                        try:
                            await handler(message)
                        except Exception as e:
                            logger.error(f"Handler error: {e}")

            except Exception as e:
                logger.error(f"Consumer error: {e}")
                await asyncio.sleep(1)


# shared/messaging/event_bus.py
from typing import Callable, Dict, List
import asyncio
import logging
from .base import MessageBroker
from ..events.base import Event

logger = logging.getLogger(__name__)

class EventBus:
    """ì´ë²¤íŠ¸ ë²„ìŠ¤ (Facade)"""

    def __init__(self, broker: MessageBroker):
        self.broker = broker
        self.handlers: Dict[str, List[Callable]] = {}

    async def start(self):
        """ì‹œì‘"""
        await self.broker.connect()
        logger.info("EventBus started")

    async def stop(self):
        """ì¢…ë£Œ"""
        await self.broker.disconnect()
        logger.info("EventBus stopped")

    async def publish(self, event: Event):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        topic = event.topic
        message = event.to_json()
        await self.broker.publish(topic, message)
        logger.debug(f"Event published: {event.event_type}")

    def subscribe(self, event_type: str, handler: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë… (ë°ì½”ë ˆì´í„°ë¡œ ì‚¬ìš© ê°€ëŠ¥)"""
        def decorator(func: Callable):
            if event_type not in self.handlers:
                self.handlers[event_type] = []
            self.handlers[event_type].append(func)
            logger.info(f"Handler registered: {event_type} -> {func.__name__}")
            return func

        if callable(handler):
            # ì§ì ‘ í˜¸ì¶œ: event_bus.subscribe("metric.collected", my_handler)
            if event_type not in self.handlers:
                self.handlers[event_type] = []
            self.handlers[event_type].append(handler)
            return handler
        else:
            # ë°ì½”ë ˆì´í„°: @event_bus.subscribe("metric.collected")
            return decorator

    async def start_consuming(self):
        """ëª¨ë“  êµ¬ë…ëœ ì´ë²¤íŠ¸ ì†Œë¹„ ì‹œì‘"""
        for event_type, handlers in self.handlers.items():
            topic = event_type.lower().replace('_', '.')

            async def handle_message(message: str):
                for handler in handlers:
                    try:
                        await handler(message)
                    except Exception as e:
                        logger.error(f"Handler error: {e}")

            await self.broker.subscribe(topic, handle_message)
```

### 1.5 Docker Compose ì„¤ì •

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Message Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data

  # Time Series Database
  influxdb:
    image: influxdb:2.7
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: adminadmin
      DOCKER_INFLUXDB_INIT_ORG: padit
      DOCKER_INFLUXDB_INIT_BUCKET: metrics
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: my-super-secret-token
    volumes:
      - influxdb_data:/var/lib/influxdb2

  # PostgreSQL (Metadata)
  postgres:
    image: postgres:15
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: padit
      POSTGRES_PASSWORD: padit123
      POSTGRES_DB: padit
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Redis (Cache & Light messaging)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # MinIO (Object Storage for models)
  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

  # Grafana (Visualization)
  grafana:
    image: grafana/grafana:10.0.0
    ports:
      - "3100:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: grafana-influxdb-datasource
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  kafka_data:
  influxdb_data:
  postgres_data:
  redis_data:
  minio_data:
  grafana_data:
```

---

## ğŸ”Œ Phase 2: ì²« ë²ˆì§¸ í”„ë¡œí† ì½œ ì–´ëŒ‘í„° êµ¬í˜„ (Week 3-4)

### 2.1 ORAN CFM ì–´ëŒ‘í„° (OCADì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜)

```python
# plugins/protocol_adapters/oran_cfm/adapter.py
from typing import AsyncIterator, Dict, Any
from datetime import datetime
import asyncio

from padit.shared.domain import Metric, MetricType

class ORANCFMAdapter:
    """ORAN CFM í”„ë¡œí† ì½œ ì–´ëŒ‘í„°"""

    @property
    def name(self) -> str:
        return "oran-cfm"

    @property
    def version(self) -> str:
        return "1.0.0"

    @property
    def description(self) -> str:
        return "ORAN CFM-Lite protocol adapter (UDP Echo, eCPRI, LBM)"

    def validate_config(self, config: Dict[str, Any]) -> bool:
        """ì„¤ì • ê²€ì¦"""
        required = ["endpoints"]
        return all(k in config for k in required)

    async def collect(self, config: Dict[str, Any]) -> AsyncIterator[Metric]:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        endpoints = config["endpoints"]
        interval = config.get("interval_sec", 10)

        while True:
            for endpoint in endpoints:
                # UDP Echo ìˆ˜ì§‘
                if endpoint.get("udp_echo", True):
                    rtt = await self._collect_udp_echo(endpoint)
                    yield Metric(
                        timestamp=datetime.utcnow(),
                        source_id=endpoint["id"],
                        metric_name="udp_echo_rtt",
                        value=rtt,
                        unit="ms",
                        metric_type=MetricType.LATENCY,
                        protocol="oran-cfm",
                        metadata={"endpoint": endpoint["host"]}
                    )

                # eCPRI delay ìˆ˜ì§‘
                if endpoint.get("ecpri", True):
                    delay = await self._collect_ecpri_delay(endpoint)
                    yield Metric(
                        timestamp=datetime.utcnow(),
                        source_id=endpoint["id"],
                        metric_name="ecpri_delay",
                        value=delay,
                        unit="us",
                        metric_type=MetricType.LATENCY,
                        protocol="oran-cfm",
                        metadata={"endpoint": endpoint["host"]}
                    )

                # LBM ìˆ˜ì§‘
                if endpoint.get("lbm", True):
                    lbm_rtt = await self._collect_lbm(endpoint)
                    yield Metric(
                        timestamp=datetime.utcnow(),
                        source_id=endpoint["id"],
                        metric_name="lbm_rtt",
                        value=lbm_rtt,
                        unit="ms",
                        metric_type=MetricType.LATENCY,
                        protocol="oran-cfm",
                        metadata={"endpoint": endpoint["host"]}
                    )

            await asyncio.sleep(interval)

    async def _collect_udp_echo(self, endpoint: Dict) -> float:
        """UDP Echo RTT ì¸¡ì •"""
        # OCAD ì½”ë“œì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜
        # ocad/collectors/udp_echo.py ë¡œì§ ì‚¬ìš©
        pass

    async def _collect_ecpri_delay(self, endpoint: Dict) -> float:
        """eCPRI delay ì¸¡ì •"""
        pass

    async def _collect_lbm(self, endpoint: Dict) -> float:
        """LBM RTT ì¸¡ì •"""
        pass


# plugins/protocol_adapters/oran_cfm/__init__.py
from .adapter import ORANCFMAdapter

def create_adapter():
    """í”ŒëŸ¬ê·¸ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    return ORANCFMAdapter()
```

### 2.2 HTTP ì–´ëŒ‘í„° (ë²”ìš©ì„± ê²€ì¦)

```python
# plugins/protocol_adapters/http/adapter.py
from typing import AsyncIterator, Dict, Any
from datetime import datetime
import asyncio
import aiohttp
import time

from padit.shared.domain import Metric, MetricType

class HTTPAdapter:
    """HTTP/HTTPS í”„ë¡œí† ì½œ ì–´ëŒ‘í„°"""

    @property
    def name(self) -> str:
        return "http"

    @property
    def version(self) -> str:
        return "1.0.0"

    def validate_config(self, config: Dict[str, Any]) -> bool:
        required = ["endpoints"]
        return all(k in config for k in required)

    async def collect(self, config: Dict[str, Any]) -> AsyncIterator[Metric]:
        """HTTP ì—”ë“œí¬ì¸íŠ¸ ëª¨ë‹ˆí„°ë§"""
        endpoints = config["endpoints"]
        interval = config.get("interval_sec", 30)
        timeout = config.get("timeout_sec", 10)

        async with aiohttp.ClientSession() as session:
            while True:
                for endpoint in endpoints:
                    url = endpoint["url"]
                    method = endpoint.get("method", "GET")

                    # Response time ì¸¡ì •
                    start = time.time()
                    try:
                        async with session.request(
                            method, url, timeout=timeout
                        ) as response:
                            elapsed_ms = (time.time() - start) * 1000
                            status = response.status

                            # Latency metric
                            yield Metric(
                                timestamp=datetime.utcnow(),
                                source_id=endpoint.get("id", url),
                                metric_name="http_response_time",
                                value=elapsed_ms,
                                unit="ms",
                                metric_type=MetricType.LATENCY,
                                protocol="http",
                                metadata={
                                    "url": url,
                                    "method": method,
                                    "status_code": status
                                }
                            )

                            # Availability metric
                            is_success = 200 <= status < 300
                            yield Metric(
                                timestamp=datetime.utcnow(),
                                source_id=endpoint.get("id", url),
                                metric_name="http_availability",
                                value=1.0 if is_success else 0.0,
                                unit="%",
                                metric_type=MetricType.AVAILABILITY,
                                protocol="http",
                                metadata={
                                    "url": url,
                                    "status_code": status
                                }
                            )

                    except asyncio.TimeoutError:
                        # Timeout
                        yield Metric(
                            timestamp=datetime.utcnow(),
                            source_id=endpoint.get("id", url),
                            metric_name="http_availability",
                            value=0.0,
                            unit="%",
                            metric_type=MetricType.AVAILABILITY,
                            protocol="http",
                            metadata={
                                "url": url,
                                "error": "timeout"
                            }
                        )

                    except Exception as e:
                        # Error
                        yield Metric(
                            timestamp=datetime.utcnow(),
                            source_id=endpoint.get("id", url),
                            metric_name="http_availability",
                            value=0.0,
                            unit="%",
                            metric_type=MetricType.AVAILABILITY,
                            protocol="http",
                            metadata={
                                "url": url,
                                "error": str(e)
                            }
                        )

                await asyncio.sleep(interval)


# plugins/protocol_adapters/http/__init__.py
from .adapter import HTTPAdapter

def create_adapter():
    return HTTPAdapter()
```

### 2.3 í”ŒëŸ¬ê·¸ì¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬

```python
# services/ingestion/plugin_registry.py
from typing import Dict, Optional
from pathlib import Path
import importlib.util
import logging

logger = logging.getLogger(__name__)

class PluginRegistry:
    """í”„ë¡œí† ì½œ ì–´ëŒ‘í„° í”ŒëŸ¬ê·¸ì¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬"""

    def __init__(self):
        self.adapters: Dict[str, Any] = {}

    def register(self, adapter):
        """ì–´ëŒ‘í„° ë“±ë¡"""
        name = adapter.name
        if name in self.adapters:
            logger.warning(f"Adapter {name} already registered, overwriting")

        self.adapters[name] = adapter
        logger.info(f"Adapter registered: {name} v{adapter.version}")

    def get(self, name: str) -> Optional[Any]:
        """ì–´ëŒ‘í„° ì¡°íšŒ"""
        return self.adapters.get(name)

    def list(self) -> Dict[str, Any]:
        """ëª¨ë“  ì–´ëŒ‘í„° ì¡°íšŒ"""
        return {
            name: {
                "version": adapter.version,
                "description": getattr(adapter, "description", "")
            }
            for name, adapter in self.adapters.items()
        }

    def discover_plugins(self, plugin_dir: Path):
        """í”ŒëŸ¬ê·¸ì¸ ìë™ ë°œê²¬ ë° ë“±ë¡"""
        if not plugin_dir.exists():
            logger.warning(f"Plugin directory not found: {plugin_dir}")
            return

        # protocol_adapters ë””ë ‰í† ë¦¬ íƒìƒ‰
        adapters_dir = plugin_dir / "protocol_adapters"
        if not adapters_dir.exists():
            return

        for adapter_dir in adapters_dir.iterdir():
            if not adapter_dir.is_dir():
                continue

            # __init__.pyì—ì„œ create_adapter í•¨ìˆ˜ ì°¾ê¸°
            init_file = adapter_dir / "__init__.py"
            if not init_file.exists():
                continue

            try:
                # ë™ì  import
                spec = importlib.util.spec_from_file_location(
                    f"plugin_{adapter_dir.name}",
                    init_file
                )
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # create_adapter() í˜¸ì¶œ
                if hasattr(module, "create_adapter"):
                    adapter = module.create_adapter()
                    self.register(adapter)
                else:
                    logger.warning(
                        f"Plugin {adapter_dir.name} missing create_adapter()"
                    )

            except Exception as e:
                logger.error(f"Failed to load plugin {adapter_dir.name}: {e}")


# ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬
registry = PluginRegistry()
```

ê³„ì†í•´ì„œ ë” êµ¬í˜„í• ê¹Œìš”? ë‹¤ìŒ ë‚´ìš©:
- Ingestion Service êµ¬í˜„
- Feature Engineering Service
- Detection Service
- ì „ì²´ ì—°ë™ ì˜ˆì œ

ì›í•˜ì‹œëŠ” ë°©í–¥ì„ ì•Œë ¤ì£¼ì‹œë©´ ê³„ì† ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤!