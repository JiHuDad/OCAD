#!/usr/bin/env python3
"""í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸.

ì‚¬ìš© ëª©ì :
1. í•™ìŠµìš© ì •ìƒ ë°ì´í„° ìƒì„± (Training set)
2. ê²€ì¦ìš© ì •ìƒ ë°ì´í„° ìƒì„± (Validation/Test set)
3. ê²€ì¦ìš© ë¹„ì •ìƒ ë°ì´í„° ìƒì„± (Anomaly test set)
"""

import argparse
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))


class DatasetGenerator:
    """ë°ì´í„°ì…‹ ìƒì„±ê¸°."""

    def __init__(self, output_dir: Path, random_seed: int = 42):
        """ì´ˆê¸°í™”.

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            random_seed: ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
        """
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        np.random.seed(random_seed)

    def generate_normal_data(
        self,
        duration_hours: int = 24,
        interval_seconds: int = 60,
        endpoint_id: str = "endpoint-1",
    ) -> pd.DataFrame:
        """ì •ìƒ ìš´ì˜ ë°ì´í„° ìƒì„± (í•™ìŠµ/ê²€ì¦ìš©).

        Args:
            duration_hours: ìƒì„±í•  ë°ì´í„° ì‹œê°„ (ì‹œê°„)
            interval_seconds: ë°ì´í„° ìˆ˜ì§‘ ê°„ê²© (ì´ˆ)
            endpoint_id: ì—”ë“œí¬ì¸íŠ¸ ID

        Returns:
            DataFrame: ì •ìƒ ë°ì´í„°
        """
        num_samples = int(duration_hours * 3600 / interval_seconds)
        base_time = datetime(2025, 10, 1, 0, 0, 0)

        print(f"\nğŸ“Š ì •ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
        print(f"   - ê¸°ê°„: {duration_hours}ì‹œê°„")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {num_samples}ê°œ")
        print(f"   - ê°„ê²©: {interval_seconds}ì´ˆ")

        data = []

        # ì •ìƒ ë²”ìœ„ì˜ ê¸°ì¤€ê°’
        udp_rtt_base = 5.0  # ms
        ecpri_base = 100.0  # us
        lbm_rtt_base = 7.0  # ms

        for i in range(num_samples):
            timestamp = base_time + timedelta(seconds=i * interval_seconds)

            # ì •ìƒ ë²”ìœ„ ë‚´ì—ì„œ ì•½ê°„ì˜ ë³€ë™ (ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ)
            udp_rtt = udp_rtt_base + np.random.normal(0, 0.3)
            ecpri = ecpri_base + np.random.normal(0, 5.0)
            lbm_rtt = lbm_rtt_base + np.random.normal(0, 0.4)

            # ì‹œê°„ëŒ€ë³„ ì•½ê°„ì˜ ë³€í™” (ì¼ì¤‘ íŒ¨í„´)
            hour = timestamp.hour
            if 8 <= hour <= 18:  # ë‚® ì‹œê°„ëŒ€ ì•½ê°„ ë†’ìŒ
                udp_rtt += 0.2
                ecpri += 3.0
                lbm_rtt += 0.1

            data.append({
                "timestamp": timestamp,
                "endpoint_id": endpoint_id,
                "udp_echo_rtt_ms": round(max(0, udp_rtt), 3),
                "ecpri_delay_us": round(max(0, ecpri), 3),
                "lbm_rtt_ms": round(max(0, lbm_rtt), 3),
                "ccm_miss_count": 0,
            })

        df = pd.DataFrame(data)
        print(f"âœ… ì •ìƒ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
        return df

    def generate_drift_anomaly(
        self,
        duration_hours: int = 12,
        interval_seconds: int = 60,
        endpoint_id: str = "endpoint-2",
    ) -> pd.DataFrame:
        """Drift (ì ì§„ì  ì¦ê°€) ì´ìƒ ë°ì´í„° ìƒì„±.

        Args:
            duration_hours: ìƒì„±í•  ë°ì´í„° ì‹œê°„ (ì‹œê°„)
            interval_seconds: ë°ì´í„° ìˆ˜ì§‘ ê°„ê²© (ì´ˆ)
            endpoint_id: ì—”ë“œí¬ì¸íŠ¸ ID

        Returns:
            DataFrame: Drift ì´ìƒ ë°ì´í„°
        """
        num_samples = int(duration_hours * 3600 / interval_seconds)
        base_time = datetime(2025, 10, 2, 0, 0, 0)

        print(f"\nğŸ“ˆ Drift ì´ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
        print(f"   - ê¸°ê°„: {duration_hours}ì‹œê°„")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {num_samples}ê°œ")

        data = []

        # ì •ìƒ ë²”ìœ„ì—ì„œ ì‹œì‘
        udp_rtt_base = 5.0
        ecpri_base = 100.0
        lbm_rtt_base = 7.0

        for i in range(num_samples):
            timestamp = base_time + timedelta(seconds=i * interval_seconds)

            # ì ì§„ì  ì¦ê°€ (ì‹œê°„ì— ë¹„ë¡€)
            drift_factor = (i / num_samples) * 3.0  # ìµœëŒ€ 3ë°°ê¹Œì§€ ì¦ê°€

            udp_rtt = (udp_rtt_base + drift_factor) + np.random.normal(0, 0.3)
            ecpri = (ecpri_base + drift_factor * 20) + np.random.normal(0, 5.0)
            lbm_rtt = (lbm_rtt_base + drift_factor * 0.5) + np.random.normal(0, 0.4)

            data.append({
                "timestamp": timestamp,
                "endpoint_id": endpoint_id,
                "udp_echo_rtt_ms": round(max(0, udp_rtt), 3),
                "ecpri_delay_us": round(max(0, ecpri), 3),
                "lbm_rtt_ms": round(max(0, lbm_rtt), 3),
                "ccm_miss_count": 0,
            })

        df = pd.DataFrame(data)
        print(f"âœ… Drift ì´ìƒ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
        return df

    def generate_spike_anomaly(
        self,
        duration_hours: int = 6,
        interval_seconds: int = 60,
        endpoint_id: str = "endpoint-3",
    ) -> pd.DataFrame:
        """Spike (ê¸‰ê²©í•œ ì¦ê°€) ì´ìƒ ë°ì´í„° ìƒì„±.

        Args:
            duration_hours: ìƒì„±í•  ë°ì´í„° ì‹œê°„ (ì‹œê°„)
            interval_seconds: ë°ì´í„° ìˆ˜ì§‘ ê°„ê²© (ì´ˆ)
            endpoint_id: ì—”ë“œí¬ì¸íŠ¸ ID

        Returns:
            DataFrame: Spike ì´ìƒ ë°ì´í„°
        """
        num_samples = int(duration_hours * 3600 / interval_seconds)
        base_time = datetime(2025, 10, 3, 0, 0, 0)

        print(f"\nâš¡ Spike ì´ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
        print(f"   - ê¸°ê°„: {duration_hours}ì‹œê°„")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {num_samples}ê°œ")

        data = []

        # ì •ìƒ ë²”ìœ„
        udp_rtt_base = 5.0
        ecpri_base = 100.0
        lbm_rtt_base = 7.0

        # ìŠ¤íŒŒì´í¬ ë°œìƒ ì§€ì  (ì¤‘ê°„ ì§€ì )
        spike_start = int(num_samples * 0.4)
        spike_end = int(num_samples * 0.6)

        for i in range(num_samples):
            timestamp = base_time + timedelta(seconds=i * interval_seconds)

            # ìŠ¤íŒŒì´í¬ êµ¬ê°„ì—ì„œ ê¸‰ê²©íˆ ì¦ê°€
            if spike_start <= i <= spike_end:
                spike_factor = 5.0  # 5ë°° ì¦ê°€
            else:
                spike_factor = 0.0

            udp_rtt = (udp_rtt_base + spike_factor) + np.random.normal(0, 0.5)
            ecpri = (ecpri_base + spike_factor * 50) + np.random.normal(0, 10.0)
            lbm_rtt = (lbm_rtt_base + spike_factor) + np.random.normal(0, 0.8)

            data.append({
                "timestamp": timestamp,
                "endpoint_id": endpoint_id,
                "udp_echo_rtt_ms": round(max(0, udp_rtt), 3),
                "ecpri_delay_us": round(max(0, ecpri), 3),
                "lbm_rtt_ms": round(max(0, lbm_rtt), 3),
                "ccm_miss_count": 0,
            })

        df = pd.DataFrame(data)
        print(f"âœ… Spike ì´ìƒ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
        return df

    def generate_packet_loss_anomaly(
        self,
        duration_hours: int = 6,
        interval_seconds: int = 60,
        endpoint_id: str = "endpoint-4",
    ) -> pd.DataFrame:
        """íŒ¨í‚· ì†ì‹¤ ì´ìƒ ë°ì´í„° ìƒì„±.

        Args:
            duration_hours: ìƒì„±í•  ë°ì´í„° ì‹œê°„ (ì‹œê°„)
            interval_seconds: ë°ì´í„° ìˆ˜ì§‘ ê°„ê²© (ì´ˆ)
            endpoint_id: ì—”ë“œí¬ì¸íŠ¸ ID

        Returns:
            DataFrame: íŒ¨í‚· ì†ì‹¤ ì´ìƒ ë°ì´í„°
        """
        num_samples = int(duration_hours * 3600 / interval_seconds)
        base_time = datetime(2025, 10, 4, 0, 0, 0)

        print(f"\nğŸ“‰ íŒ¨í‚· ì†ì‹¤ ì´ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
        print(f"   - ê¸°ê°„: {duration_hours}ì‹œê°„")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {num_samples}ê°œ")

        data = []

        udp_rtt_base = 5.0
        ecpri_base = 100.0
        lbm_rtt_base = 7.0

        # íŒ¨í‚· ì†ì‹¤ ì‹œì‘ ì§€ì 
        loss_start = int(num_samples * 0.5)

        for i in range(num_samples):
            timestamp = base_time + timedelta(seconds=i * interval_seconds)

            udp_rtt = udp_rtt_base + np.random.normal(0, 0.3)
            ecpri = ecpri_base + np.random.normal(0, 5.0)
            lbm_rtt = lbm_rtt_base + np.random.normal(0, 0.4)

            # íŒ¨í‚· ì†ì‹¤ ë°œìƒ
            ccm_miss = 0
            if i >= loss_start:
                # 10-30% í™•ë¥ ë¡œ íŒ¨í‚· ì†ì‹¤
                if np.random.random() < 0.2:
                    ccm_miss = np.random.randint(1, 5)

            data.append({
                "timestamp": timestamp,
                "endpoint_id": endpoint_id,
                "udp_echo_rtt_ms": round(max(0, udp_rtt), 3),
                "ecpri_delay_us": round(max(0, ecpri), 3),
                "lbm_rtt_ms": round(max(0, lbm_rtt), 3),
                "ccm_miss_count": ccm_miss,
            })

        df = pd.DataFrame(data)
        print(f"âœ… íŒ¨í‚· ì†ì‹¤ ì´ìƒ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
        return df

    def save_dataset(self, df: pd.DataFrame, filename: str, formats: list[str] = None):
        """ë°ì´í„°ì…‹ ì €ì¥.

        Args:
            df: DataFrame
            filename: íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
            formats: ì €ì¥ í˜•ì‹ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ['csv', 'parquet'])
        """
        if formats is None:
            formats = ['csv', 'parquet']

        for fmt in formats:
            if fmt == 'csv':
                path = self.output_dir / f"{filename}.csv"
                df.to_csv(path, index=False)
                print(f"   ğŸ’¾ CSV ì €ì¥: {path}")
            elif fmt == 'parquet':
                path = self.output_dir / f"{filename}.parquet"
                df.to_parquet(path, index=False)
                print(f"   ğŸ’¾ Parquet ì €ì¥: {path}")
            elif fmt == 'excel':
                path = self.output_dir / f"{filename}.xlsx"
                df.to_excel(path, index=False)
                print(f"   ğŸ’¾ Excel ì €ì¥: {path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(
        description="í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("data/datasets"),
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--training-hours",
        type=int,
        default=24,
        help="í•™ìŠµ ë°ì´í„° ìƒì„± ì‹œê°„ (ì‹œê°„)",
    )
    parser.add_argument(
        "--validation-hours",
        type=int,
        default=12,
        help="ê²€ì¦ ì •ìƒ ë°ì´í„° ìƒì„± ì‹œê°„ (ì‹œê°„)",
    )
    parser.add_argument(
        "--anomaly-hours",
        type=int,
        default=6,
        help="ë¹„ì •ìƒ ë°ì´í„° ìƒì„± ì‹œê°„ (ì‹œê°„)",
    )
    parser.add_argument(
        "--interval",
        type=int,
        default=60,
        help="ë°ì´í„° ìˆ˜ì§‘ ê°„ê²© (ì´ˆ)",
    )
    parser.add_argument(
        "--formats",
        nargs='+',
        default=['csv', 'parquet'],
        choices=['csv', 'parquet', 'excel'],
        help="ì €ì¥ í˜•ì‹",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)",
    )

    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ“¦ OCAD ë°ì´í„°ì…‹ ìƒì„±ê¸°")
    print("=" * 70)
    print(f"\nì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(f"ì €ì¥ í˜•ì‹: {', '.join(args.formats)}")
    print(f"ëœë¤ ì‹œë“œ: {args.seed}")

    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = DatasetGenerator(args.output_dir, random_seed=args.seed)

    # 1. í•™ìŠµìš© ì •ìƒ ë°ì´í„° ìƒì„±
    print("\n" + "=" * 70)
    print("1ï¸âƒ£ í•™ìŠµìš© ì •ìƒ ë°ì´í„° (Training Set)")
    print("=" * 70)
    training_normal = generator.generate_normal_data(
        duration_hours=args.training_hours,
        interval_seconds=args.interval,
        endpoint_id="training-endpoint",
    )
    generator.save_dataset(training_normal, "01_training_normal", args.formats)

    # 2. ê²€ì¦ìš© ì •ìƒ ë°ì´í„° ìƒì„±
    print("\n" + "=" * 70)
    print("2ï¸âƒ£ ê²€ì¦ìš© ì •ìƒ ë°ì´í„° (Validation Normal Set)")
    print("=" * 70)
    validation_normal = generator.generate_normal_data(
        duration_hours=args.validation_hours,
        interval_seconds=args.interval,
        endpoint_id="validation-endpoint",
    )
    generator.save_dataset(validation_normal, "02_validation_normal", args.formats)

    # 3. ê²€ì¦ìš© ë¹„ì •ìƒ ë°ì´í„° - Drift
    print("\n" + "=" * 70)
    print("3ï¸âƒ£ ê²€ì¦ìš© ë¹„ì •ìƒ ë°ì´í„° - Drift (ì ì§„ì  ì¦ê°€)")
    print("=" * 70)
    validation_drift = generator.generate_drift_anomaly(
        duration_hours=args.anomaly_hours,
        interval_seconds=args.interval,
    )
    generator.save_dataset(validation_drift, "03_validation_drift_anomaly", args.formats)

    # 4. ê²€ì¦ìš© ë¹„ì •ìƒ ë°ì´í„° - Spike
    print("\n" + "=" * 70)
    print("4ï¸âƒ£ ê²€ì¦ìš© ë¹„ì •ìƒ ë°ì´í„° - Spike (ê¸‰ê²©í•œ ì¦ê°€)")
    print("=" * 70)
    validation_spike = generator.generate_spike_anomaly(
        duration_hours=args.anomaly_hours,
        interval_seconds=args.interval,
    )
    generator.save_dataset(validation_spike, "04_validation_spike_anomaly", args.formats)

    # 5. ê²€ì¦ìš© ë¹„ì •ìƒ ë°ì´í„° - íŒ¨í‚· ì†ì‹¤
    print("\n" + "=" * 70)
    print("5ï¸âƒ£ ê²€ì¦ìš© ë¹„ì •ìƒ ë°ì´í„° - íŒ¨í‚· ì†ì‹¤")
    print("=" * 70)
    validation_loss = generator.generate_packet_loss_anomaly(
        duration_hours=args.anomaly_hours,
        interval_seconds=args.interval,
    )
    generator.save_dataset(validation_loss, "05_validation_packet_loss_anomaly", args.formats)

    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 70)
    print("âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nìƒì„±ëœ íŒŒì¼ ëª©ë¡:")
    print(f"   1. í•™ìŠµìš© ì •ìƒ ë°ì´í„°: {len(training_normal)}ê°œ ë ˆì½”ë“œ")
    print(f"   2. ê²€ì¦ìš© ì •ìƒ ë°ì´í„°: {len(validation_normal)}ê°œ ë ˆì½”ë“œ")
    print(f"   3. Drift ì´ìƒ ë°ì´í„°: {len(validation_drift)}ê°œ ë ˆì½”ë“œ")
    print(f"   4. Spike ì´ìƒ ë°ì´í„°: {len(validation_spike)}ê°œ ë ˆì½”ë“œ")
    print(f"   5. íŒ¨í‚· ì†ì‹¤ ì´ìƒ ë°ì´í„°: {len(validation_loss)}ê°œ ë ˆì½”ë“œ")

    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ:")
    print(f"      python scripts/prepare_timeseries_data_v2.py \\")
    print(f"          --input-csv {args.output_dir}/01_training_normal.csv \\")
    print(f"          --output-dir data/processed --metric-type udp_echo")
    print(f"      python scripts/train_tcn_model.py --metric-type udp_echo")
    print(f"")
    print(f"   2. ê²€ì¦ ë°ì´í„°ë¡œ ì¶”ë¡  ì‹¤í–‰:")
    print(f"      python scripts/inference_simple.py \\")
    print(f"          --input {args.output_dir}/02_validation_normal.csv \\")
    print(f"          --output results_normal.csv")
    print(f"      python scripts/inference_simple.py \\")
    print(f"          --input {args.output_dir}/03_validation_drift_anomaly.csv \\")
    print(f"          --output results_drift.csv")


if __name__ == "__main__":
    main()
