#!/usr/bin/env python3
"""ì¶”ë¡  ì‹¤í–‰ + ë³´ê³ ì„œ ìƒì„± í†µí•© ìŠ¤í¬ë¦½íŠ¸.

í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì¶”ë¡  ì‹¤í–‰ê³¼ ë³´ê³ ì„œ ìƒì„±ì„ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import argparse
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from ocad.core.data_source import DataSourceFactory
from ocad.core.logging import get_logger

logger = get_logger(__name__)


class InferenceRunner:
    """ì¶”ë¡  ì‹¤í–‰ í´ë˜ìŠ¤."""

    def __init__(self, model_path: Path, config: dict):
        """ì´ˆê¸°í™”.

        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            config: ì¶”ë¡  ì„¤ì •
        """
        self.model_path = model_path
        self.config = config
        self.threshold = config.get("threshold", 0.5)
        self.rule_threshold = config.get("rule_threshold", 10.0)
        logger.info("InferenceRunner ì´ˆê¸°í™” ì™„ë£Œ", config=config)

    def run(self, data_source, output_path: Path = None):
        """ì¶”ë¡  ì‹¤í–‰.

        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ (DataSource ì¸í„°í˜ì´ìŠ¤)
            output_path: ê²°ê³¼ ì¶œë ¥ ê²½ë¡œ (ì„ íƒ)

        Returns:
            DataFrame: ì¶”ë¡  ê²°ê³¼
        """
        print(f"\n{'=' * 70}")
        print(f"ì¶”ë¡  ì‹¤í–‰")
        print(f"{'=' * 70}")

        # ë°ì´í„° ì†ŒìŠ¤ ë©”íƒ€ë°ì´í„° ì¶œë ¥
        metadata = data_source.get_metadata()
        print(f"\në°ì´í„° ì†ŒìŠ¤ ì •ë³´:")
        for key, value in metadata.items():
            if key == "label_distribution":
                print(f"  {key}:")
                for label, count in value.items():
                    print(f"    {label}: {count}ê°œ")
            else:
                print(f"  {key}: {value}")

        # ë°ì´í„° ìˆ˜ì§‘ ë° ì¶”ë¡ 
        all_results = []
        print(f"\nì¶”ë¡  ì‹¤í–‰ ì¤‘...")

        batch_count = 0
        total_processed = 0

        for batch in data_source:
            batch_count += 1
            # batch.metricsê°€ ì‹¤ì œ MetricData ë¦¬ìŠ¤íŠ¸
            total_processed += len(batch.metrics)

            # ê° ë ˆì½”ë“œì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰
            for metric in batch.metrics:
                # metricì€ ì´ë¯¸ dict í˜•íƒœ (FileDataSourceì˜ ê²½ìš°)
                result = self._detect_anomaly(metric)
                all_results.append(result)

            # ì§„í–‰ ìƒí™© ì¶œë ¥ (100ê°œ ë°°ì¹˜ë§ˆë‹¤)
            if batch_count % 5 == 0:
                print(f"  ë°°ì¹˜ {batch_count}: {total_processed}ê°œ ì²˜ë¦¬ë¨")

        print(f"\nâœ… ì´ {total_processed}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ")

        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        results_df = pd.DataFrame(all_results)

        # ê²°ê³¼ ì €ì¥
        if output_path:
            results_df.to_csv(output_path, index=False)
            file_size_kb = output_path.stat().st_size / 1024
            print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size_kb:.2f} KB")

        # ê²°ê³¼ ë¶„ì„ ì¶œë ¥
        self._print_analysis(results_df)

        return results_df

    def _detect_anomaly(self, record: dict) -> dict:
        """ë‹¨ì¼ ë ˆì½”ë“œì— ëŒ€í•´ ì´ìƒ íƒì§€ ìˆ˜í–‰ (ë£° ê¸°ë°˜).

        Args:
            record: ë°ì´í„° ë ˆì½”ë“œ

        Returns:
            dict: ì¶”ë¡  ê²°ê³¼
        """
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ (FileDataSourceëŠ” _ms, _us ì—†ì´ ë°˜í™˜)
        udp_rtt = record.get("udp_echo_rtt", record.get("udp_echo_rtt_ms", 0))
        ecpri_delay = record.get("ecpri_delay", record.get("ecpri_delay_us", 0))
        lbm_rtt = record.get("lbm_rtt", record.get("lbm_rtt_ms", 0))

        # ë£° ê¸°ë°˜ íƒì§€
        rule_based_score = 1.0 if udp_rtt > self.rule_threshold else 0.0
        ecpri_score = 1.0 if ecpri_delay > 200 else 0.0
        lbm_score = 1.0 if lbm_rtt > self.rule_threshold else 0.0

        # ì•™ìƒë¸” ì ìˆ˜ (í‰ê· )
        composite_score = (rule_based_score + ecpri_score + lbm_score) / 3.0

        # ì˜ˆì¸¡ ë¼ë²¨
        predicted_label = "anomaly" if composite_score >= self.threshold else "normal"

        return {
            "timestamp": record.get("timestamp"),
            "endpoint_id": record.get("endpoint_id"),
            "udp_echo_rtt": udp_rtt,
            "ecpri_delay": ecpri_delay,
            "lbm_rtt": lbm_rtt,
            "label": record.get("label", "unknown"),
            "rule_based_score": rule_based_score,
            "ecpri_score": ecpri_score,
            "lbm_score": lbm_score,
            "composite_score": composite_score,
            "predicted_label": predicted_label,
        }

    def _print_analysis(self, results_df: pd.DataFrame):
        """ê²°ê³¼ ë¶„ì„ ì¶œë ¥."""
        print(f"\n{'=' * 70}")
        print(f"ê²°ê³¼ ë¶„ì„")
        print(f"{'=' * 70}")

        # ì˜ˆì¸¡ ë¶„í¬
        print(f"\nì˜ˆì¸¡ ë¶„í¬:")
        pred_counts = results_df["predicted_label"].value_counts()
        for label, count in pred_counts.items():
            percentage = count / len(results_df) * 100
            print(f"  {label}: {count}ê°œ ({percentage:.1f}%)")

        # ì •í™•ë„ ê³„ì‚° (label ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
        if "label" in results_df.columns and results_df["label"].notna().any():
            correct = (results_df["predicted_label"] == results_df["label"]).sum()
            accuracy = correct / len(results_df) * 100
            print(f"\nì •í™•ë„: {accuracy:.2f}%")

            # Confusion Matrix
            print(f"\nConfusion Matrix:")
            cm = pd.crosstab(
                results_df["label"],
                results_df["predicted_label"],
                rownames=["ì‹¤ì œ"],
                colnames=["ì˜ˆì¸¡"],
            )
            print(cm)

        # íƒì§€ê¸°ë³„ í‰ê·  ì ìˆ˜
        print(f"\níƒì§€ê¸°ë³„ í‰ê·  ì ìˆ˜:")
        for col in ["rule_based", "ecpri", "lbm", "composite"]:
            score_col = f"{col}_score"
            if score_col in results_df.columns:
                mean_score = results_df[score_col].mean()
                print(f"  {col:15s}: {mean_score:.3f}")


def generate_report(input_file: Path, output_file: Path = None):
    """ì¶”ë¡  ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±.

    Args:
        input_file: ì¶”ë¡  ê²°ê³¼ CSV íŒŒì¼
        output_file: ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ (ì„ íƒ, ê¸°ë³¸ê°’: reports/inference_report_<timestamp>.md)
    """
    import numpy as np
    from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

    print(f"\n{'=' * 70}")
    print(f"OCAD ì¶”ë¡  ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±")
    print(f"{'=' * 70}")
    print(f"ì…ë ¥: {input_file}")

    # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„± (ë‚ ì§œ ì‹œê°„ í¬í•¨)
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = Path("reports") / f"inference_report_{timestamp}.md"

    output_file.parent.mkdir(parents=True, exist_ok=True)
    print(f"ì¶œë ¥: {output_file}")

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(input_file)

    # ë³´ê³ ì„œ ìƒì„±
    lines = []

    # í—¤ë”
    lines.append("# OCAD ì¶”ë¡  ê²°ê³¼ ë³´ê³ ì„œ")
    lines.append("")
    lines.append(f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"**ì…ë ¥ íŒŒì¼**: {input_file}")
    lines.append(f"**ì´ ë ˆì½”ë“œ**: {len(df)}ê°œ")
    lines.append("")
    lines.append("---")
    lines.append("")

    # ì „ì²´ ìš”ì•½
    lines.append("## ğŸ“Š ì „ì²´ ìš”ì•½")
    lines.append("")

    # ì‹œê°„ ë²”ìœ„
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    start_time = df["timestamp"].min()
    end_time = df["timestamp"].max()
    duration = end_time - start_time

    lines.append(f"- **ë¶„ì„ ê¸°ê°„**: {start_time} ~ {end_time}")
    lines.append(f"- **ë¶„ì„ ì‹œê°„**: {duration}")
    lines.append("")

    # ì—”ë“œí¬ì¸íŠ¸ ìˆ˜
    endpoints = df["endpoint_id"].value_counts()
    lines.append(f"- **ì—”ë“œí¬ì¸íŠ¸ ìˆ˜**: {len(endpoints)}ê°œ")
    for endpoint, count in endpoints.items():
        lines.append(f"  - `{endpoint}`: {count}ê°œ ë ˆì½”ë“œ")
    lines.append("")

    # ì‹¤ì œ ë¼ë²¨ ë¶„í¬
    if "label" in df.columns and df["label"].notna().any():
        lines.append("### ì‹¤ì œ ë¼ë²¨ ë¶„í¬")
        lines.append("")
        label_counts = df["label"].value_counts()
        for label, count in label_counts.items():
            percentage = count / len(df) * 100
            lines.append(f"- **{label}**: {count}ê°œ ({percentage:.1f}%)")
        lines.append("")

    # ì˜ˆì¸¡ ë¼ë²¨ ë¶„í¬
    lines.append("### ì˜ˆì¸¡ ë¼ë²¨ ë¶„í¬")
    lines.append("")
    pred_counts = df["predicted_label"].value_counts()
    for label, count in pred_counts.items():
        percentage = count / len(df) * 100
        lines.append(f"- **{label}**: {count}ê°œ ({percentage:.1f}%)")
    lines.append("")
    lines.append("---")
    lines.append("")

    # ì„±ëŠ¥ ì§€í‘œ
    if "label" in df.columns and df["label"].notna().any():
        lines.append("## ğŸ¯ ì„±ëŠ¥ ì§€í‘œ")
        lines.append("")

        # ì •í™•ë„
        correct = (df["predicted_label"] == df["label"]).sum()
        accuracy = correct / len(df) * 100
        lines.append(f"### ì •í™•ë„: **{accuracy:.2f}%**")
        lines.append("")

        # Confusion Matrix
        lines.append("### Confusion Matrix")
        lines.append("")
        lines.append("```")
        cm = pd.crosstab(
            df["label"],
            df["predicted_label"],
            rownames=["ì‹¤ì œ"],
            colnames=["ì˜ˆì¸¡"],
        )
        lines.append(str(cm))
        lines.append("```")
        lines.append("")

        # Precision, Recall, F1
        try:
            # ì´ì§„ ë¶„ë¥˜ ì§€í‘œ ê³„ì‚°
            y_true = (df["label"] == "anomaly").astype(int)
            y_pred = (df["predicted_label"] == "anomaly").astype(int)

            if y_pred.sum() > 0:  # anomaly ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš°
                precision = precision_score(y_true, y_pred, zero_division=0) * 100
                recall = recall_score(y_true, y_pred, zero_division=0) * 100
                f1 = f1_score(y_true, y_pred, zero_division=0) * 100

                lines.append("### ì„¸ë¶€ ì„±ëŠ¥ ì§€í‘œ")
                lines.append("")
                lines.append(f"- **Precision (ì •ë°€ë„)**: {precision:.2f}%")
                lines.append("  - ì´ìƒìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì´ìƒ ë¹„ìœ¨")
                lines.append(f"- **Recall (ì¬í˜„ìœ¨)**: {recall:.2f}%")
                lines.append("  - ì‹¤ì œ ì´ìƒ ì¤‘ íƒì§€í•œ ë¹„ìœ¨")
                lines.append(f"- **F1 Score**: {f1:.2f}%")
                lines.append("  - Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· ")
                lines.append("")

            # TP, FP, TN, FN
            cm_array = confusion_matrix(y_true, y_pred, labels=[0, 1])
            tn, fp, fn, tp = cm_array.ravel() if cm_array.size == 4 else (0, 0, 0, 0)

            lines.append("### ë¶„ë¥˜ ê²°ê³¼ ìƒì„¸")
            lines.append("")
            lines.append(f"- **True Positive (TP)**: {tp}ê°œ - âœ… ì´ìƒì„ ì´ìƒìœ¼ë¡œ ì •í™•íˆ íƒì§€")
            lines.append(f"- **False Positive (FP)**: {fp}ê°œ - âš ï¸ ì •ìƒì„ ì´ìƒìœ¼ë¡œ ì˜¤íƒ")
            lines.append(f"- **True Negative (TN)**: {tn}ê°œ - âœ… ì •ìƒì„ ì •ìƒìœ¼ë¡œ ì •í™•íˆ ë¶„ë¥˜")
            lines.append(f"- **False Negative (FN)**: {fn}ê°œ - âŒ ì´ìƒì„ ì •ìƒìœ¼ë¡œ ë¯¸íƒ")
            lines.append("")

        except Exception as e:
            lines.append(f"âš ï¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            lines.append("")

        lines.append("---")
        lines.append("")

    # íƒì§€ê¸°ë³„ ë¶„ì„
    lines.append("## ğŸ” íƒì§€ê¸°ë³„ ë¶„ì„")
    lines.append("")

    for detector_name, score_col in [
        ("Rule Based", "rule_based_score"),
        ("Ecpri", "ecpri_score"),
        ("Lbm", "lbm_score"),
        ("Composite", "composite_score"),
    ]:
        if score_col in df.columns:
            scores = df[score_col]
            mean_score = scores.mean()
            max_score = scores.max()
            active_count = (scores > 0).sum()
            active_pct = active_count / len(df) * 100

            lines.append(f"### {detector_name}")
            lines.append("")
            lines.append(f"- **í‰ê·  ì ìˆ˜**: {mean_score:.3f}")
            lines.append(f"- **ìµœëŒ€ ì ìˆ˜**: {max_score:.3f}")
            lines.append(f"- **í™œì„±í™” íšŸìˆ˜**: {active_count}íšŒ ({active_pct:.1f}%)")

            if active_count > 0:
                active_mean = scores[scores > 0].mean()
                lines.append(f"- **í™œì„±í™” ì‹œ í‰ê·  ì ìˆ˜**: {active_mean:.3f}")

            lines.append("")

    lines.append("---")
    lines.append("")

    # ë©”íŠ¸ë¦­ í†µê³„
    lines.append("## ğŸ“ˆ ë©”íŠ¸ë¦­ í†µê³„")
    lines.append("")

    for metric_name, col_name in [
        ("UDP ECHO RTT", "udp_echo_rtt"),
        ("ECPRI DELAY", "ecpri_delay"),
        ("LBM RTT", "lbm_rtt"),
    ]:
        if col_name in df.columns:
            values = df[col_name]
            lines.append(f"### {metric_name}")
            lines.append("")
            lines.append(f"- **í‰ê· **: {values.mean():.2f}")
            lines.append(f"- **í‘œì¤€í¸ì°¨**: {values.std():.2f}")
            lines.append(f"- **ìµœì†Œê°’**: {values.min():.2f}")
            lines.append(f"- **ìµœëŒ€ê°’**: {values.max():.2f}")
            lines.append(f"- **ì¤‘ì•™ê°’**: {values.median():.2f}")
            lines.append(f"- **P95**: {values.quantile(0.95):.2f}")
            lines.append(f"- **P99**: {values.quantile(0.99):.2f}")
            lines.append("")

    lines.append("---")
    lines.append("")

    # False Negative ë¶„ì„
    if "label" in df.columns and df["label"].notna().any():
        fn_df = df[(df["label"] == "anomaly") & (df["predicted_label"] == "normal")]

        if len(fn_df) > 0:
            lines.append("## âŒ False Negative ë¶„ì„ (ë¯¸íƒì§€)")
            lines.append("")
            lines.append(f"**ì´ {len(fn_df)}ê±´ì˜ ì´ìƒì´ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**")
            lines.append("")

            # ë¯¸íƒì§€ ë©”íŠ¸ë¦­ ë²”ìœ„
            lines.append("### ë¯¸íƒì§€ ë©”íŠ¸ë¦­ ë²”ìœ„")
            lines.append("")
            for metric_name, col_name in [
                ("udp_echo_rtt", "udp_echo_rtt"),
                ("ecpri_delay", "ecpri_delay"),
                ("lbm_rtt", "lbm_rtt"),
            ]:
                if col_name in fn_df.columns:
                    values = fn_df[col_name]
                    lines.append(
                        f"- **{metric_name}**: {values.min():.2f} ~ {values.max():.2f} (í‰ê· : {values.mean():.2f})"
                    )
            lines.append("")

            # ë¯¸íƒì§€ ì‚¬ë¡€ ìƒ˜í”Œ
            lines.append("### ë¯¸íƒì§€ ì‚¬ë¡€ (ì²˜ìŒ 10ê°œ)")
            lines.append("")
            lines.append("| ì‹œê°„ | UDP RTT | eCPRI Delay | LBM RTT | Composite Score |")
            lines.append("|------|---------|-------------|---------|-----------------|")

            for _, row in fn_df.head(10).iterrows():
                time_str = row["timestamp"].strftime("%H:%M:%S") if isinstance(row["timestamp"], pd.Timestamp) else str(row["timestamp"])
                lines.append(
                    f"| {time_str} | {row['udp_echo_rtt']:.2f} | "
                    f"{row['ecpri_delay']:.2f} | {row['lbm_rtt']:.2f} | "
                    f"{row['composite_score']:.3f} |"
                )

            lines.append("")
            lines.append("---")
            lines.append("")

    # Top ì´ìƒ ì¼€ì´ìŠ¤
    top_anomalies = df.nlargest(10, "composite_score")

    lines.append("## âš ï¸ íƒì§€ëœ ì´ìƒ ì¼€ì´ìŠ¤ (ìƒìœ„ 10ê°œ)")
    lines.append("")
    lines.append("ê°€ì¥ ë†’ì€ ì´ìƒ ì ìˆ˜ë¥¼ ê¸°ë¡í•œ ì¼€ì´ìŠ¤ë“¤:")
    lines.append("")
    lines.append("| ìˆœìœ„ | ì‹œê°„ | UDP RTT | eCPRI Delay | LBM RTT | Composite Score | ì‹¤ì œ ë¼ë²¨ |")
    lines.append("|------|------|---------|-------------|---------|-----------------|-----------|")

    for idx, (_, row) in enumerate(top_anomalies.iterrows(), 1):
        time_str = row["timestamp"].strftime("%H:%M:%S") if isinstance(row["timestamp"], pd.Timestamp) else str(row["timestamp"])
        label = row.get("label", "unknown")
        label_emoji = "âœ…" if label == "anomaly" else "âŒ" if label == "normal" else "â“"
        lines.append(
            f"| {idx} | {time_str} | {row['udp_echo_rtt']:.2f} | "
            f"{row['ecpri_delay']:.2f} | {row['lbm_rtt']:.2f} | "
            f"{row['composite_score']:.3f} | {label} {label_emoji} |"
        )

    lines.append("")
    lines.append("---")
    lines.append("")

    # ì‹œê°„ëŒ€ë³„ ë¶„ì„
    lines.append("## ğŸ“… ì‹œê°„ëŒ€ë³„ ë¶„ì„")
    lines.append("")
    lines.append("### 5ë¶„ ê°„ê²© ì´ìƒ íƒì§€ ë¹ˆë„")
    lines.append("")

    df["time_bin"] = df["timestamp"].dt.floor("5min")
    time_groups = df.groupby("time_bin")

    for time_bin, group in time_groups:
        anomaly_count = (group["predicted_label"] == "anomaly").sum()
        avg_score = group["composite_score"].mean()
        time_str = time_bin.strftime("%H:%M")

        # ê°„ë‹¨í•œ ì‹œê°í™” (â–ˆ ê°œìˆ˜)
        bar_length = min(int(anomaly_count / 2), 15)
        bar = "â–ˆ" * bar_length if bar_length > 0 else "â–ˆ"

        lines.append(f"`{time_str}` {bar} {anomaly_count}ê±´ (í‰ê·  ì ìˆ˜: {avg_score:.3f})")

    lines.append("")
    lines.append("---")
    lines.append("")

    # ê¶Œì¥ ì‚¬í•­
    if "label" in df.columns and df["label"].notna().any():
        y_true = (df["label"] == "anomaly").astype(int)
        y_pred = (df["predicted_label"] == "anomaly").astype(int)

        fp_rate = (y_pred - y_true).clip(lower=0).sum() / len(df) * 100
        fn_rate = (y_true - y_pred).clip(lower=0).sum() / y_true.sum() * 100 if y_true.sum() > 0 else 0

        lines.append("## ğŸ’¡ ê¶Œì¥ ì‚¬í•­")
        lines.append("")

        if fn_rate > 20:
            lines.append("### âš ï¸ ë†’ì€ False Negativeìœ¨")
            lines.append("")
            lines.append(f"- í˜„ì¬ **{fn_rate:.1f}%**ì˜ ì´ìƒì´ íƒì§€ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.")
            lines.append("- **ê¶Œì¥ ì¡°ì¹˜**:")
            lines.append("  1. íƒì§€ ì„ê³„ê°’ ë‚®ì¶”ê¸° (`--threshold 0.3`)")
            lines.append("  2. ë£° ê¸°ë°˜ ì„ê³„ê°’ ì¡°ì • (`--rule-threshold 8.0`)")
            lines.append("  3. ë³€í™”ì  íƒì§€ê¸° ì¶”ê°€ ê³ ë ¤")
        elif fp_rate > 5:
            lines.append("### âš ï¸ ë†’ì€ False Positiveìœ¨")
            lines.append("")
            lines.append(f"- í˜„ì¬ **{fp_rate:.1f}%**ì˜ ì •ìƒ ë°ì´í„°ê°€ ì˜¤íƒë˜ê³  ìˆìŠµë‹ˆë‹¤.")
            lines.append("- **ê¶Œì¥ ì¡°ì¹˜**:")
            lines.append("  1. íƒì§€ ì„ê³„ê°’ ë†’ì´ê¸° (`--threshold 0.7`)")
            lines.append("  2. ë£° ê¸°ë°˜ ì„ê³„ê°’ ì™„í™” (`--rule-threshold 12.0`)")
        else:
            lines.append("### âœ… ì–‘í˜¸í•œ íƒì§€ ì„±ëŠ¥")
            lines.append("")
            lines.append("í˜„ì¬ íƒì§€ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤:")
            lines.append(f"- False Negativeìœ¨: {fn_rate:.1f}% (ëª©í‘œ: â‰¤ 20%)")
            lines.append(f"- False Positiveìœ¨: {fp_rate:.1f}% (ëª©í‘œ: â‰¤ 5%)")

        lines.append("")
        lines.append("---")
        lines.append("")

    # ë¶€ë¡
    lines.append("## ğŸ“ ë¶€ë¡")
    lines.append("")
    lines.append("### íŒŒì¼ ì •ë³´")
    lines.append("")
    lines.append(f"- **ë³´ê³ ì„œ íŒŒì¼**: {output_file}")
    lines.append(f"- **ë°ì´í„° íŒŒì¼**: {input_file}")
    lines.append(f"- **ìƒì„± ì‹œê°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")

    # íŒŒì¼ ì €ì¥
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    file_size_kb = output_file.stat().st_size / 1024
    print(f"\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {output_file}")
    print(f"   íŒŒì¼ í¬ê¸°: {file_size_kb:.2f} KB")
    print(f"\në³´ê³ ì„œ í™•ì¸:")
    print(f"  cat {output_file}")
    print(f"  code {output_file}")
    print(f"{'=' * 70}")


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(
        description="ì¶”ë¡  ì‹¤í–‰ + ë³´ê³ ì„œ ìƒì„± (í†µí•©)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # ë°ì´í„° ì†ŒìŠ¤
    parser.add_argument(
        "--data-source",
        type=str,
        required=True,
        help="ë°ì´í„° ì†ŒìŠ¤ ê²½ë¡œ (CSV/Excel/Parquet íŒŒì¼)",
    )

    # ëª¨ë¸ ê²½ë¡œ
    parser.add_argument(
        "--model-path",
        type=Path,
        default=Path("ocad/models/tcn"),
        help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ",
    )

    # ì¶”ë¡  ì„¤ì •
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="ì´ìƒ íƒì§€ ì„ê³„ê°’ (0.0 ~ 1.0)",
    )

    parser.add_argument(
        "--rule-threshold",
        type=float,
        default=10.0,
        help="ë£° ê¸°ë°˜ íƒì§€ ì„ê³„ê°’ (ms)",
    )

    # ì¶œë ¥ ì„¤ì •
    parser.add_argument(
        "--output-csv",
        type=Path,
        default=None,
        help="ì¶”ë¡  ê²°ê³¼ CSV íŒŒì¼ (ê¸°ë³¸ê°’: data/inference_results_<timestamp>.csv)",
    )

    parser.add_argument(
        "--output-report",
        type=Path,
        default=None,
        help="ë³´ê³ ì„œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ (ê¸°ë³¸ê°’: reports/inference_report_<timestamp>.md)",
    )

    # ë°ì´í„° ë°°ì¹˜ í¬ê¸°
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="ë°ì´í„° ì†ŒìŠ¤ ë°°ì¹˜ í¬ê¸°",
    )

    args = parser.parse_args()

    # ì‹œì‘ ì‹œê°„
    start_time = datetime.now()
    print(f"{'=' * 70}")
    print(f"OCAD ì¶”ë¡  + ë³´ê³ ì„œ ìƒì„± (í†µí•©)")
    print(f"{'=' * 70}")
    print(f"ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"\nì„¤ì •:")
    print(f"  ë°ì´í„° ì†ŒìŠ¤: {args.data_source}")
    print(f"  ëª¨ë¸ ê²½ë¡œ: {args.model_path}")
    print(f"  ì„ê³„ê°’: {args.threshold}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"{'=' * 70}")

    # ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„± (ë‚ ì§œ ì‹œê°„ í¬í•¨)
    timestamp = start_time.strftime("%Y%m%d_%H%M%S")

    if args.output_csv is None:
        args.output_csv = Path("data") / f"inference_results_{timestamp}.csv"

    if args.output_report is None:
        args.output_report = Path("reports") / f"inference_report_{timestamp}.md"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    args.output_csv.parent.mkdir(parents=True, exist_ok=True)
    args.output_report.parent.mkdir(parents=True, exist_ok=True)

    # ë°ì´í„° ì†ŒìŠ¤ ìƒì„±
    data_source = DataSourceFactory.create_from_file(
        args.data_source,
        batch_size=args.batch_size,
    )
    logger.info("ë°ì´í„° ì†ŒìŠ¤ ìƒì„± ì™„ë£Œ")
    print(f"\nâœ… ë°ì´í„° ì†ŒìŠ¤ ìƒì„± ì™„ë£Œ")

    # ì¶”ë¡  ì‹¤í–‰
    config = {
        "threshold": args.threshold,
        "rule_threshold": args.rule_threshold,
    }
    runner = InferenceRunner(args.model_path, config)

    try:
        results_df = runner.run(data_source, output_path=args.output_csv)
    finally:
        # ë°ì´í„° ì†ŒìŠ¤ ì¢…ë£Œ
        data_source.close()

    # ë³´ê³ ì„œ ìƒì„±
    print(f"\n{'=' * 70}")
    generate_report(args.output_csv, args.output_report)

    # ì¢…ë£Œ ì‹œê°„
    end_time = datetime.now()
    duration = end_time - start_time

    print(f"\n{'=' * 70}")
    print(f"âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"{'=' * 70}")
    print(f"ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì†Œìš” ì‹œê°„: {duration}")
    print(f"\nìƒì„±ëœ íŒŒì¼:")
    print(f"  - ì¶”ë¡  ê²°ê³¼: {args.output_csv}")
    print(f"  - ë³´ê³ ì„œ: {args.output_report}")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  - ê²°ê³¼ í™•ì¸: head -20 {args.output_csv}")
    print(f"  - ë³´ê³ ì„œ í™•ì¸: cat {args.output_report}")
    print(f"{'=' * 70}")


if __name__ == "__main__":
    main()
