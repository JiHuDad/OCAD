# ì¶”ë¡  ê°€ì´ë“œ

**ëª©ì **: í•™ìŠµëœ ëª¨ë¸ë¡œ ì´ìƒ íƒì§€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œ

**ëŒ€ìƒ**: ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí•˜ê³  ì‹¤ì œ ë°ì´í„°ì—ì„œ ì´ìƒì„ íƒì§€í•˜ë ¤ëŠ” ì‚¬ìš©ì

---

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### 1. í™˜ê²½ ì„¤ì •
```bash
# Python venv í™œì„±í™”
source .venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜ í™•ì¸
pip list | grep -E "pandas|numpy|scikit-learn"
```

### 2. í•™ìŠµëœ ëª¨ë¸ (í–¥í›„)
```bash
# ëª¨ë¸ íŒŒì¼ í™•ì¸
ls -lh ocad/models/tcn/
ls -lh ocad/models/isolation_forest/
```

**í˜„ì¬**: ë£° ê¸°ë°˜ íƒì§€ë§Œ ì‚¬ìš© (ëª¨ë¸ ë¡œë“œ ë¶ˆí•„ìš”)
**í–¥í›„**: TCN, Isolation Forest ëª¨ë¸ í†µí•©

### 3. ì¶”ë¡  ë°ì´í„°
```bash
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
python scripts/generate_training_inference_data.py --mode inference
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### Step 1: ì¶”ë¡  í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±

```bash
python scripts/generate_training_inference_data.py --mode inference
```

**ìƒì„± íŒŒì¼**: `data/inference_test_scenarios.csv`
- ì´ 780ê°œ ë ˆì½”ë“œ
- ì •ìƒ: 478ê°œ (61.3%)
- ì´ìƒ: 302ê°œ (38.7%)
- 6ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ í¬í•¨

### Step 2: ì¶”ë¡  ì‹¤í–‰

```bash
python scripts/run_inference.py \
    --data-source data/inference_test_scenarios.csv \
    --output data/inference_results.csv
```

**ì˜ˆìƒ ì¶œë ¥**:
```
======================================================================
OCAD ì¶”ë¡  ì‹¤í–‰ (ë°ì´í„° ì†ŒìŠ¤ ì¶”ìƒí™”)
======================================================================
ì‹œì‘ ì‹œê°„: 2025-10-27 07:57:48

ì„¤ì •:
  ë°ì´í„° ì†ŒìŠ¤: data/inference_test_scenarios.csv
  ëª¨ë¸ ê²½ë¡œ: ocad/models/tcn
  ì„ê³„ê°’: 0.5
  ë°°ì¹˜ í¬ê¸°: 100
======================================================================

âœ… ë°ì´í„° ì†ŒìŠ¤ ìƒì„± ì™„ë£Œ

ë°ì´í„° ì†ŒìŠ¤ ì •ë³´:
  source_type: file
  total_records: 780
  batch_size: 100
  start_time: 2025-10-15 12:00:00
  end_time: 2025-10-15 14:09:50
  label_distribution: {'normal': 478, 'anomaly': 302}

ì¶”ë¡  ì‹¤í–‰ ì¤‘...
  ë°°ì¹˜ 5: 500ê°œ ì²˜ë¦¬ë¨

âœ… ì´ 780ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ

======================================================================
ê²°ê³¼ ë¶„ì„
======================================================================

ì˜ˆì¸¡ ë¶„í¬:
  normal: 565ê°œ (72.4%)
  anomaly: 215ê°œ (27.6%)

ì •í™•ë„: 88.85%

Confusion Matrix:
ì˜ˆì¸¡       anomaly  normal
ì‹¤ì œ
anomaly      215      87
normal         0     478

íƒì§€ê¸°ë³„ í‰ê·  ì ìˆ˜:
  rule_based     : 0.329
  ecpri          : 0.286
  lbm            : 0.254
  composite      : 0.290

âœ… ê²°ê³¼ ì €ì¥: data/inference_results.csv
======================================================================
```

### Step 3: ê²°ê³¼ í™•ì¸

```bash
# ì²˜ìŒ 20ê°œ ë ˆì½”ë“œ í™•ì¸
head -20 data/inference_results.csv

# ì´ìƒ íƒì§€ëœ ì¼€ì´ìŠ¤ë§Œ í™•ì¸
grep ",anomaly$" data/inference_results.csv | head -10
```

---

## ğŸ“Š ê²°ê³¼ ë¶„ì„

### ê²°ê³¼ íŒŒì¼ êµ¬ì¡°

**data/inference_results.csv**:
```csv
timestamp,endpoint_id,udp_echo_rtt,ecpri_delay,lbm_rtt,label,rule_based_score,ecpri_score,lbm_score,composite_score,predicted_label
1760529600000,o-ru-test-001,4.76,93.93,7.33,normal,0.0,0.0,0.0,0.0,normal
1760532080000,o-ru-test-001,12.50,180.45,18.23,anomaly,1.0,0.0,1.0,0.666667,anomaly
```

**ì»¬ëŸ¼ ì„¤ëª…**:
- `timestamp`: Unix timestamp (ms)
- `endpoint_id`: ì—”ë“œí¬ì¸íŠ¸ ID
- `udp_echo_rtt`: UDP Echo RTT (ms)
- `ecpri_delay`: eCPRI delay (Âµs)
- `lbm_rtt`: LBM RTT (ms)
- `label`: ì‹¤ì œ ë¼ë²¨ (normal/anomaly)
- `rule_based_score`: ë£° ê¸°ë°˜ ì ìˆ˜ (0-1)
- `ecpri_score`: eCPRI ì ìˆ˜ (0-1)
- `lbm_score`: LBM ì ìˆ˜ (0-1)
- `composite_score`: ì¢…í•© ì ìˆ˜ (0-1)
- `predicted_label`: ì˜ˆì¸¡ ë¼ë²¨ (normal/anomaly)

### Pythonìœ¼ë¡œ ë¶„ì„

```python
import pandas as pd
import matplotlib.pyplot as plt

# ê²°ê³¼ ë¡œë“œ
df = pd.read_csv('data/inference_results.csv')

# 1. ì˜ˆì¸¡ ë¶„í¬
print(df['predicted_label'].value_counts())
# normal    565
# anomaly   215

# 2. ì •í™•ë„
accuracy = (df['label'] == df['predicted_label']).mean() * 100
print(f'ì •í™•ë„: {accuracy:.2f}%')  # 88.85%

# 3. Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(df['label'], df['predicted_label'])
print(cm)
# [[478   0]    # ì •ìƒì„ ì •ìƒìœ¼ë¡œ: 478, ì •ìƒì„ ì´ìƒìœ¼ë¡œ: 0
#  [ 87 215]]   # ì´ìƒì„ ì •ìƒìœ¼ë¡œ: 87,  ì´ìƒì„ ì´ìƒìœ¼ë¡œ: 215

# 4. ì„±ëŠ¥ ì§€í‘œ
from sklearn.metrics import precision_recall_fscore_support
precision, recall, f1, _ = precision_recall_fscore_support(
    df['label'], df['predicted_label'], average='binary', pos_label='anomaly'
)
print(f'Precision: {precision:.2%}')  # 100.00% (False Positive 0ê°œ)
print(f'Recall: {recall:.2%}')        # 71.19% (215/302)
print(f'F1 Score: {f1:.2%}')          # 83.14%

# 5. False Negative ë¶„ì„
false_negatives = df[(df['label'] == 'anomaly') & (df['predicted_label'] == 'normal')]
print(f'False Negative: {len(false_negatives)}ê°œ')
print(false_negatives[['timestamp', 'udp_echo_rtt', 'ecpri_delay', 'lbm_rtt', 'composite_score']].head(10))

# 6. ì‹œê³„ì—´ ì‹œê°í™”
df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='ms')
plt.figure(figsize=(15, 5))
plt.plot(df['timestamp_dt'], df['udp_echo_rtt'], label='UDP Echo RTT', alpha=0.7)
plt.scatter(df[df['predicted_label'] == 'anomaly']['timestamp_dt'],
            df[df['predicted_label'] == 'anomaly']['udp_echo_rtt'],
            color='red', label='Anomaly', s=50)
plt.axhline(y=10, color='r', linestyle='--', label='Threshold')
plt.legend()
plt.xlabel('Time')
plt.ylabel('UDP Echo RTT (ms)')
plt.title('Anomaly Detection Results')
plt.show()
```

---

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ì„ê³„ê°’ ì¡°ì •

```bash
# ê¸°ë³¸ ì„ê³„ê°’ (ë³´ìˆ˜ì )
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --threshold 0.5

# ë¯¼ê°í•˜ê²Œ (ë” ë§ì´ íƒì§€)
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --threshold 0.3 \
    --rule-threshold 8.0

# ë³´ìˆ˜ì ìœ¼ë¡œ (ì˜¤íƒ ìµœì†Œí™”)
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --threshold 0.7 \
    --rule-threshold 15.0
```

**íŒŒë¼ë¯¸í„° ì„¤ëª…**:
- `--threshold`: ì¢…í•© ì ìˆ˜ ì„ê³„ê°’ (0.0-1.0)
- `--rule-threshold`: ë£° ê¸°ë°˜ ì„ê³„ê°’ (ms)

### 2. ë°°ì¹˜ í¬ê¸° ì¡°ì •

```bash
# ì‘ì€ ë°°ì¹˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --batch-size 50

# í° ë°°ì¹˜ (ì²˜ë¦¬ ì†ë„ í–¥ìƒ)
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --batch-size 500
```

### 3. ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹

```bash
# CSV íŒŒì¼
python scripts/run_inference.py \
    --data-source data/metrics.csv

# Excel íŒŒì¼
python scripts/run_inference.py \
    --data-source data/metrics.xlsx

# Parquet íŒŒì¼ (ëŒ€ìš©ëŸ‰)
python scripts/run_inference.py \
    --data-source data/metrics.parquet
```

### 4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (í–¥í›„)

```bash
# Kafka
python scripts/run_inference.py \
    --streaming \
    --kafka-broker localhost:9092 \
    --kafka-topic oran-metrics \
    --output results_stream.csv

# WebSocket
python scripts/run_inference.py \
    --streaming \
    --websocket-url ws://oran-server:8080/metrics
```

---

## ğŸ¯ ì„±ëŠ¥ íŠœë‹

### False Negative ì¤„ì´ê¸°

**ë¬¸ì œ**: í˜„ì¬ 87ê°œì˜ False Negative (ì´ìƒì„ ì •ìƒìœ¼ë¡œ ì˜¤íŒ)

**ì›ì¸ ë¶„ì„**:
```python
# False Negative ë ˆì½”ë“œ í™•ì¸
false_negatives = df[(df['label'] == 'anomaly') & (df['predicted_label'] == 'normal')]
print(false_negatives[['udp_echo_rtt', 'composite_score']].describe())

# ëŒ€ë¶€ë¶„ 5-10ms ë²”ìœ„ (ì„ê³„ê°’ 10ms ë¯¸ë§Œ)
```

**í•´ê²°ì±…**:

#### 1. ì„ê³„ê°’ ë‚®ì¶”ê¸°
```bash
python scripts/run_inference.py \
    --data-source data/inference_test_scenarios.csv \
    --rule-threshold 8.0 \
    --threshold 0.3
```

#### 2. ë³€í™”ì  íƒì§€ê¸° ì¶”ê°€
- CUSUM ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì ì§„ì  ë³€í™” íƒì§€
- Drift ì´ˆê¸° ë‹¨ê³„ í¬ì°©

#### 3. ë‹¤ë³€ëŸ‰ íƒì§€ í™œìš©
- ì—¬ëŸ¬ ë©”íŠ¸ë¦­ ë™ì‹œ ê³ ë ¤
- UDP RTTëŠ” ë‚®ì§€ë§Œ eCPRI delayê°€ ë†’ì€ ê²½ìš° íƒì§€

### False Positive ìµœì†Œí™”

**í˜„ì¬**: False Positive 0ê°œ (ë§¤ìš° ë³´ìˆ˜ì )

**ìœ ì§€ ë°©ë²•**:
- ì„ê³„ê°’ì„ ë„ˆë¬´ ë‚®ì¶”ì§€ ì•Šê¸°
- ì—¬ëŸ¬ íƒì§€ê¸°ì˜ í•©ì˜ ìš”êµ¬
- Hold-down timer ì ìš© (ì¼ì‹œì  spike ë¬´ì‹œ)

---

## ğŸ“ˆ ì‹œë‚˜ë¦¬ì˜¤ë³„ íƒì§€ ì„±ëŠ¥

### 1. Normal (ì •ìƒ ìš´ì˜)
```
ë°ì´í„°: 180ê°œ
íƒì§€ ê²°ê³¼: ëª¨ë‘ ì •ìƒ (100%)
```

### 2. Drift (ì ì§„ì  ì¦ê°€)
```
ë°ì´í„°: 180ê°œ
íƒì§€ ê²°ê³¼:
  - ì´ˆê¸° (5â†’10ms): ë¯¸íƒì§€ (ì„ê³„ê°’ ë¯¸ë§Œ)
  - ì¤‘ê¸° (10â†’15ms): íƒì§€ ì‹œì‘
  - í›„ê¸° (15â†’20ms): ëª¨ë‘ íƒì§€
í‰ê·  íƒì§€ìœ¨: ~70%
```

**ê°œì„  ë°©ë²•**: ë³€í™”ì  íƒì§€ê¸° (CUSUM) ì¶”ê°€

### 3. Spike (ê¸‰ê²©í•œ ì¼ì‹œì  ì¦ê°€)
```
ë°ì´í„°: 120ê°œ
íƒì§€ ê²°ê³¼: Spike ë°œìƒ ì‹œ ëª¨ë‘ íƒì§€ (100%)
```

### 4. Jitter (ë¶ˆê·œì¹™ ë³€ë™)
```
ë°ì´í„°: 120ê°œ
íƒì§€ ê²°ê³¼:
  - í° ë³€ë™: íƒì§€
  - ì‘ì€ ë³€ë™: ë¯¸íƒì§€
í‰ê·  íƒì§€ìœ¨: ~60%
```

### 5. Multi-metric Failure (ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ì¥ì• )
```
ë°ì´í„°: 90ê°œ
íƒì§€ ê²°ê³¼: ëª¨ë‘ íƒì§€ (100%)
  - UDP + eCPRI + LBM ë™ì‹œ ì´ìƒ
```

### 6. Recovery (ë³µêµ¬)
```
ë°ì´í„°: 90ê°œ
íƒì§€ ê²°ê³¼:
  - ì´ìƒ êµ¬ê°„: íƒì§€
  - ë³µêµ¬ êµ¬ê°„: ì •ìƒ íŒì •
```

---

## ğŸ” ë””ë²„ê¹… ê°€ì´ë“œ

### 1. ì¶”ë¡ ì´ ë„ˆë¬´ ëŠë¦¼

**ì›ì¸**: ë°°ì¹˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ

**í•´ê²°**:
```bash
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --batch-size 500  # ê¸°ë³¸ê°’ 100ì—ì„œ ì¦ê°€
```

### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì›ì¸**: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì „ì²´ ë¡œë“œ

**í•´ê²°**:
```bash
# íŒŒì¼ ë¶„í• 
split -l 10000 data/large_metrics.csv data/chunk_

# ì²­í¬ë³„ ì¶”ë¡ 
for file in data/chunk_*; do
    python scripts/run_inference.py --data-source $file --output "results_${file}.csv"
done
```

### 3. ì •í™•ë„ê°€ ë„ˆë¬´ ë‚®ìŒ

**ì›ì¸**:
- ì„ê³„ê°’ ë¶€ì ì ˆ
- í•™ìŠµ ë°ì´í„°ì™€ ì¶”ë¡  ë°ì´í„° ë¶„í¬ ë‹¤ë¦„
- ëª¨ë¸ ë¯¸í†µí•© (í˜„ì¬ ë£° ê¸°ë°˜ë§Œ ì‚¬ìš©)

**í•´ê²°**:
```bash
# 1. ì„ê³„ê°’ ì¡°ì •
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --threshold 0.3

# 2. ì¬í•™ìŠµ
python scripts/train_model.py \
    --data-source data/new_training_data.csv

# 3. ì—¬ëŸ¬ íƒì§€ê¸° ê²°í•© (í–¥í›„)
```

### 4. False Positiveê°€ ë§ìŒ

**ì›ì¸**: ì„ê³„ê°’ì´ ë„ˆë¬´ ë‚®ìŒ

**í•´ê²°**:
```bash
python scripts/run_inference.py \
    --data-source data/metrics.csv \
    --threshold 0.7 \
    --rule-threshold 15.0
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [Overview.md](Overview.md) - í•™ìŠµ-ì¶”ë¡  ê°œìš”
- [Training-Guide.md](Training-Guide.md) - í•™ìŠµ ê°€ì´ë“œ
- [Model-Architecture.md](Model-Architecture.md) - ëª¨ë¸ ì•„í‚¤í…ì²˜
- [Data-Source-Guide.md](../03-data-management/Data-Source-Guide.md) - ë°ì´í„° ì†ŒìŠ¤ ê°€ì´ë“œ
- [Training-Inference-Workflow.md](../02-user-guides/Training-Inference-Workflow.md) - ì „ì²´ ì›Œí¬í”Œë¡œìš°

---

## â“ FAQ

### Q1: ì¶”ë¡  ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?

**A**:
- 780ê°œ ë ˆì½”ë“œ: < 1ì´ˆ
- 10,000ê°œ ë ˆì½”ë“œ: ~2-3ì´ˆ
- 100,000ê°œ ë ˆì½”ë“œ: ~20-30ì´ˆ

### Q2: ì‹¤ì‹œê°„ ì¶”ë¡ ì´ ê°€ëŠ¥í•œê°€ìš”?

**A**: ë„¤, í–¥í›„ Kafka/WebSocket í†µí•© ì‹œ ì‹¤ì‹œê°„ ì¶”ë¡  ê°€ëŠ¥í•©ë‹ˆë‹¤.

### Q3: ëª¨ë¸ ì¬í•™ìŠµì€ ì–¸ì œ í•˜ë‚˜ìš”?

**A**:
- ORAN ì¥ë¹„ ë³€ê²½ ì‹œ
- ì •ìƒ íŒ¨í„´ ë³€í™” ì‹œ
- ì •í™•ë„ ì €í•˜ ì‹œ

### Q4: ì—¬ëŸ¬ íŒŒì¼ì„ í•œë²ˆì— ì¶”ë¡ í•  ìˆ˜ ìˆë‚˜ìš”?

**A**: í˜„ì¬ëŠ” ê°œë³„ ì‹¤í–‰ í•„ìš”. í–¥í›„ ë°°ì¹˜ ì¶”ë¡  ê¸°ëŠ¥ ì¶”ê°€ ì˜ˆì •.

```bash
# ì„ì‹œ ë°©ë²•: ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°˜ë³µ
for file in data/*.csv; do
    python scripts/run_inference.py --data-source "$file" --output "results_$(basename $file)"
done
```

### Q5: GPUë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?

**A**: í˜„ì¬ëŠ” CPUë§Œ ì‚¬ìš©. í–¥í›„ TCN/LSTM í†µí•© ì‹œ GPU ì§€ì› ì˜ˆì •.

---

**ì‘ì„±ì**: Claude Code
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-27
**ë²„ì „**: 1.0.0
