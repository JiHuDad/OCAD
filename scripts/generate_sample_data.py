#!/usr/bin/env python3
"""ÏÇ¨ÎûåÏù¥ ÏùΩÏùÑ Ïàò ÏûàÎäî Îã§ÏñëÌïú ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ïä§ÌÅ¨Î¶ΩÌä∏.

Îã§ÏñëÌïú ÏãúÎÇòÎ¶¨Ïò§ÏôÄ ÌòïÏãùÏúºÎ°ú ÏÉòÌîå Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§:
- ÏùºÍ∞Ñ/Ï£ºÍ∞Ñ/ÏõîÍ∞Ñ Îç∞Ïù¥ÌÑ∞
- Ï†ïÏÉÅ Ïö¥ÏòÅ vs Ïû•Ïï† ÏãúÎÇòÎ¶¨Ïò§
- Îã®Ïùº Ïû•ÎπÑ vs Ïó¨Îü¨ Ïû•ÎπÑ
- CSV, Excel, Parquet ÌòïÏãù
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import sys

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Î•º Í≤ΩÎ°úÏóê Ï∂îÍ∞Ä
sys.path.insert(0, str(Path(__file__).parent.parent))


class SampleDataGenerator:
    """ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±Í∏∞."""

    def __init__(self, output_dir: Path):
        """Ï¥àÍ∏∞Ìôî.

        Args:
            output_dir: Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨
        """
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def generate_normal_operation(
        self,
        endpoint_id: str = "o-ru-001",
        site_name: str = "Tower-A",
        zone: str = "Urban",
        duration_hours: int = 24,
        interval_seconds: int = 10
    ) -> pd.DataFrame:
        """Ï†ïÏÉÅ Ïö¥ÏòÅ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±.

        Args:
            endpoint_id: ÏóîÎìúÌè¨Ïù∏Ìä∏ ID
            site_name: ÏÇ¨Ïù¥Ìä∏ Ïù¥Î¶Ñ
            zone: ÏßÄÏó≠
            duration_hours: ÏÉùÏÑ±Ìï† Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞Ñ (ÏãúÍ∞Ñ)
            interval_seconds: Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Í∞ÑÍ≤© (Ï¥à)

        Returns:
            DataFrame: ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞
        """
        num_samples = int(duration_hours * 3600 / interval_seconds)
        base_time = datetime(2025, 10, 22, 0, 0, 0)

        data = []

        # Ï†ïÏÉÅ Î≤îÏúÑ Í∞í
        udp_rtt_base = 5.0  # ms
        ecpri_base = 100.0  # us
        lbm_rtt_base = 7.0  # ms

        for i in range(num_samples):
            timestamp = base_time + timedelta(seconds=i * interval_seconds)

            # Ï†ïÏÉÅ Î≤îÏúÑ ÎÇ¥ÏóêÏÑú ÏïΩÍ∞ÑÏùò Î≥ÄÎèô (¬±10%)
            udp_rtt = udp_rtt_base + np.random.normal(0, 0.3)
            ecpri = ecpri_base + np.random.normal(0, 5.0)
            lbm_rtt = lbm_rtt_base + np.random.normal(0, 0.4)

            data.append({
                "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "endpoint_id": endpoint_id,
                "site_name": site_name,
                "zone": zone,
                "udp_echo_rtt_ms": round(max(0, udp_rtt), 2),
                "ecpri_delay_us": round(max(0, ecpri), 2),
                "lbm_rtt_ms": round(max(0, lbm_rtt), 2),
                "lbm_success": True,
                "ccm_interval_ms": 1000,
                "ccm_miss_count": 0,
                "notes": "Ï†ïÏÉÅ Ïö¥ÏòÅ"
            })

        return pd.DataFrame(data)

    def generate_drift_anomaly(
        self,
        endpoint_id: str = "o-ru-002",
        site_name: str = "Tower-B",
        zone: str = "Rural"
    ) -> pd.DataFrame:
        """Drift (Ï†êÏßÑÏ†Å Ï¶ùÍ∞Ä) Ïù¥ÏÉÅ Ìå®ÌÑ¥ ÏÉùÏÑ±.

        Args:
            endpoint_id: ÏóîÎìúÌè¨Ïù∏Ìä∏ ID
            site_name: ÏÇ¨Ïù¥Ìä∏ Ïù¥Î¶Ñ
            zone: ÏßÄÏó≠

        Returns:
            DataFrame: ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞
        """
        base_time = datetime(2025, 10, 22, 12, 0, 0)
        data = []

        # Phase 1: Ï†ïÏÉÅ (30Î∂Ñ)
        for i in range(180):  # 10Ï¥à Í∞ÑÍ≤©
            timestamp = base_time + timedelta(seconds=i * 10)
            data.append({
                "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "endpoint_id": endpoint_id,
                "site_name": site_name,
                "zone": zone,
                "udp_echo_rtt_ms": round(5.0 + np.random.normal(0, 0.3), 2),
                "ecpri_delay_us": round(100.0 + np.random.normal(0, 5.0), 2),
                "lbm_rtt_ms": round(7.0 + np.random.normal(0, 0.4), 2),
                "lbm_success": True,
                "ccm_interval_ms": 1000,
                "ccm_miss_count": 0,
                "notes": "Ï†ïÏÉÅ Ïö¥ÏòÅ"
            })

        # Phase 2: Drift ÏãúÏûë (1ÏãúÍ∞Ñ)
        for i in range(360):
            timestamp = base_time + timedelta(seconds=(180 + i) * 10)
            progress = i / 360.0  # 0 to 1

            # Ï†êÏßÑÏ†ÅÏúºÎ°ú Ï¶ùÍ∞Ä
            udp_rtt = 5.0 + (progress * 15.0) + np.random.normal(0, 0.5)
            ecpri = 100.0 + (progress * 200.0) + np.random.normal(0, 10.0)
            lbm_rtt = 7.0 + (progress * 15.0) + np.random.normal(0, 0.6)

            status = "Drift ÏßÑÌñâ Ï§ë" if progress > 0.3 else "ÏïΩÍ∞Ñ Ï¶ùÍ∞Ä"

            data.append({
                "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "endpoint_id": endpoint_id,
                "site_name": site_name,
                "zone": zone,
                "udp_echo_rtt_ms": round(udp_rtt, 2),
                "ecpri_delay_us": round(ecpri, 2),
                "lbm_rtt_ms": round(lbm_rtt, 2),
                "lbm_success": progress < 0.8,
                "ccm_interval_ms": 1000,
                "ccm_miss_count": 1 if progress > 0.7 else 0,
                "notes": status
            })

        # Phase 3: Î≥µÍµ¨ (30Î∂Ñ)
        for i in range(180):
            timestamp = base_time + timedelta(seconds=(540 + i) * 10)
            progress = 1.0 - (i / 180.0)  # 1 to 0

            udp_rtt = 5.0 + (progress * 15.0) + np.random.normal(0, 0.5)
            ecpri = 100.0 + (progress * 200.0) + np.random.normal(0, 10.0)
            lbm_rtt = 7.0 + (progress * 15.0) + np.random.normal(0, 0.6)

            status = "Î≥µÍµ¨ Ï§ë" if progress > 0.2 else "Ï†ïÏÉÅ Î≥µÍµ¨"

            data.append({
                "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "endpoint_id": endpoint_id,
                "site_name": site_name,
                "zone": zone,
                "udp_echo_rtt_ms": round(udp_rtt, 2),
                "ecpri_delay_us": round(ecpri, 2),
                "lbm_rtt_ms": round(lbm_rtt, 2),
                "lbm_success": True,
                "ccm_interval_ms": 1000,
                "ccm_miss_count": 0,
                "notes": status
            })

        return pd.DataFrame(data)

    def generate_spike_anomaly(
        self,
        endpoint_id: str = "o-ru-003",
        site_name: str = "Tower-C",
        zone: str = "Suburban"
    ) -> pd.DataFrame:
        """Spike (Í∏âÍ≤©Ìïú ÏùºÏãúÏ†Å Ï¶ùÍ∞Ä) Ïù¥ÏÉÅ Ìå®ÌÑ¥ ÏÉùÏÑ±."""
        base_time = datetime(2025, 10, 22, 15, 0, 0)
        data = []

        for i in range(360):  # 1ÏãúÍ∞Ñ
            timestamp = base_time + timedelta(seconds=i * 10)

            # Îß§ 10Î∂ÑÎßàÎã§ spike Î∞úÏÉù
            is_spike = (i % 60 == 30)

            if is_spike:
                udp_rtt = 20.0 + np.random.normal(0, 2.0)
                ecpri = 300.0 + np.random.normal(0, 20.0)
                lbm_rtt = 22.0 + np.random.normal(0, 2.0)
                notes = "üö® Spike Î∞úÏÉù"
            else:
                udp_rtt = 5.0 + np.random.normal(0, 0.3)
                ecpri = 100.0 + np.random.normal(0, 5.0)
                lbm_rtt = 7.0 + np.random.normal(0, 0.4)
                notes = "Ï†ïÏÉÅ"

            data.append({
                "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "endpoint_id": endpoint_id,
                "site_name": site_name,
                "zone": zone,
                "udp_echo_rtt_ms": round(udp_rtt, 2),
                "ecpri_delay_us": round(ecpri, 2),
                "lbm_rtt_ms": round(lbm_rtt, 2),
                "lbm_success": not is_spike,
                "ccm_interval_ms": 1000,
                "ccm_miss_count": 1 if is_spike else 0,
                "notes": notes
            })

        return pd.DataFrame(data)

    def generate_multi_endpoint_data(self) -> pd.DataFrame:
        """Ïó¨Îü¨ ÏóîÎìúÌè¨Ïù∏Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±."""
        endpoints = [
            ("o-ru-001", "Tower-A", "Urban"),
            ("o-ru-002", "Tower-B", "Rural"),
            ("o-ru-003", "Tower-C", "Suburban"),
            ("o-du-001", "Datacenter-A", "Urban"),
        ]

        all_data = []

        for endpoint_id, site_name, zone in endpoints:
            # Í∞Å ÏóîÎìúÌè¨Ïù∏Ìä∏ÎßàÎã§ 1ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞
            df = self.generate_normal_operation(
                endpoint_id=endpoint_id,
                site_name=site_name,
                zone=zone,
                duration_hours=1,
                interval_seconds=30
            )
            all_data.append(df)

        return pd.concat(all_data, ignore_index=True).sort_values("timestamp")

    def save_as_csv(self, df: pd.DataFrame, filename: str):
        """CSVÎ°ú Ï†ÄÏû•."""
        output_path = self.output_dir / filename
        df.to_csv(output_path, index=False, encoding="utf-8")
        print(f"‚úÖ CSV Ï†ÄÏû•: {output_path}")
        print(f"   Î†àÏΩîÎìú Ïàò: {len(df):,}Í∞ú")
        print(f"   ÌååÏùº ÌÅ¨Í∏∞: {output_path.stat().st_size / 1024:.2f} KB")

    def save_as_excel(self, df: pd.DataFrame, filename: str, sheet_name: str = "Î©îÌä∏Î¶≠ Îç∞Ïù¥ÌÑ∞"):
        """ExcelÎ°ú Ï†ÄÏû•."""
        output_path = self.output_dir / filename

        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)

        print(f"‚úÖ Excel Ï†ÄÏû•: {output_path}")
        print(f"   Î†àÏΩîÎìú Ïàò: {len(df):,}Í∞ú")
        print(f"   ÌååÏùº ÌÅ¨Í∏∞: {output_path.stat().st_size / 1024:.2f} KB")

    def save_as_parquet(self, df: pd.DataFrame, filename: str):
        """ParquetÎ°ú Ï†ÄÏû•."""
        output_path = self.output_dir / filename
        df.to_parquet(output_path, engine="pyarrow", compression="snappy")
        print(f"‚úÖ Parquet Ï†ÄÏû•: {output_path}")
        print(f"   Î†àÏΩîÎìú Ïàò: {len(df):,}Í∞ú")
        print(f"   ÌååÏùº ÌÅ¨Í∏∞: {output_path.stat().st_size / 1024:.2f} KB")


def main():
    """Î©îÏù∏ Ìï®Ïàò."""
    print("\n" + "=" * 60)
    print("ÏÇ¨ÎûåÏù¥ ÏùΩÏùÑ Ïàò ÏûàÎäî ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±")
    print("=" * 60)

    output_dir = Path(__file__).parent.parent / "data" / "samples"
    generator = SampleDataGenerator(output_dir)

    # 1. Ï†ïÏÉÅ Ïö¥ÏòÅ Îç∞Ïù¥ÌÑ∞ (24ÏãúÍ∞Ñ)
    print("\n[1/6] Ï†ïÏÉÅ Ïö¥ÏòÅ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï§ë (24ÏãúÍ∞Ñ)...")
    df_normal = generator.generate_normal_operation(
        duration_hours=24,
        interval_seconds=60  # 1Î∂Ñ Í∞ÑÍ≤©
    )
    generator.save_as_csv(df_normal, "01_normal_operation_24h.csv")

    # 2. Drift Ïù¥ÏÉÅ Ìå®ÌÑ¥
    print("\n[2/6] Drift Ïù¥ÏÉÅ Ìå®ÌÑ¥ ÏÉùÏÑ± Ï§ë...")
    df_drift = generator.generate_drift_anomaly()
    generator.save_as_csv(df_drift, "02_drift_anomaly.csv")
    generator.save_as_excel(df_drift, "02_drift_anomaly.xlsx")

    # 3. Spike Ïù¥ÏÉÅ Ìå®ÌÑ¥
    print("\n[3/6] Spike Ïù¥ÏÉÅ Ìå®ÌÑ¥ ÏÉùÏÑ± Ï§ë...")
    df_spike = generator.generate_spike_anomaly()
    generator.save_as_csv(df_spike, "03_spike_anomaly.csv")

    # 4. Ïó¨Îü¨ ÏóîÎìúÌè¨Ïù∏Ìä∏ Îç∞Ïù¥ÌÑ∞
    print("\n[4/6] Ïó¨Îü¨ ÏóîÎìúÌè¨Ïù∏Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï§ë...")
    df_multi = generator.generate_multi_endpoint_data()
    generator.save_as_csv(df_multi, "04_multi_endpoint.csv")
    generator.save_as_parquet(df_multi, "04_multi_endpoint.parquet")

    # 5. Ï£ºÍ∞Ñ Îç∞Ïù¥ÌÑ∞ (1Ï£ºÏùº, 5Î∂Ñ Í∞ÑÍ≤©)
    print("\n[5/6] Ï£ºÍ∞Ñ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï§ë (1Ï£ºÏùº)...")
    df_weekly = generator.generate_normal_operation(
        duration_hours=24 * 7,
        interval_seconds=300  # 5Î∂Ñ Í∞ÑÍ≤©
    )
    generator.save_as_parquet(df_weekly, "05_weekly_data.parquet")

    # 6. Ï¢ÖÌï© ÏòàÏ†ú (Ï†ïÏÉÅ + Drift + Spike)
    print("\n[6/6] Ï¢ÖÌï© ÏòàÏ†ú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï§ë...")
    df_normal_short = generator.generate_normal_operation(
        endpoint_id="o-ru-001",
        duration_hours=2,
        interval_seconds=30
    )
    df_comprehensive = pd.concat([
        df_normal_short,
        df_drift,
        df_spike
    ], ignore_index=True).sort_values("timestamp").reset_index(drop=True)

    generator.save_as_excel(df_comprehensive, "06_comprehensive_example.xlsx")

    print("\n" + "=" * 60)
    print("‚úÖ Î™®Îì† ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏôÑÎ£å!")
    print("=" * 60)
    print(f"\nüìÅ Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨: {output_dir}")
    print("\nÏÉùÏÑ±Îêú ÌååÏùº:")
    print("  1. 01_normal_operation_24h.csv      - Ï†ïÏÉÅ Ïö¥ÏòÅ (24ÏãúÍ∞Ñ)")
    print("  2. 02_drift_anomaly.csv/.xlsx       - Drift Ïù¥ÏÉÅ Ìå®ÌÑ¥")
    print("  3. 03_spike_anomaly.csv             - Spike Ïù¥ÏÉÅ Ìå®ÌÑ¥")
    print("  4. 04_multi_endpoint.csv/.parquet   - Ïó¨Îü¨ ÏóîÎìúÌè¨Ïù∏Ìä∏")
    print("  5. 05_weekly_data.parquet           - Ï£ºÍ∞Ñ Îç∞Ïù¥ÌÑ∞ (7Ïùº)")
    print("  6. 06_comprehensive_example.xlsx   - Ï¢ÖÌï© ÏòàÏ†ú")
    print("\nüí° ÏÇ¨Ïö© Î∞©Î≤ï:")
    print("  - CSV ÌååÏùº: Excel, ÌÖçÏä§Ìä∏ ÏóêÎîîÌÑ∞Î°ú Ïó¥Îûå")
    print("  - Excel ÌååÏùº: Microsoft Excel, LibreOfficeÎ°ú Ïó¥Îûå")
    print("  - Parquet ÌååÏùº: ÌååÏù¥Ïç¨ pandasÎ°ú Ïó¥Îûå")
    print("    >>> import pandas as pd")
    print("    >>> df = pd.read_parquet('05_weekly_data.parquet')")
    print("    >>> print(df.head())")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå Ïò§Î•ò Î∞úÏÉù: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
