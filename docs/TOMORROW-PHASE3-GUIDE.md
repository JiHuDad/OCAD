# Phase 3 ì‘ì—… ê°€ì´ë“œ (ë‚´ì¼ ì§„í–‰)

**ë‚ ì§œ**: 2025-10-31 ì˜ˆì •  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1-2ì‹œê°„  
**ëª©í‘œ**: Isolation Forest ë‹¤ë³€ëŸ‰ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ

---

## ğŸ“‹ ì‘ì—… ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. í˜„ì¬ ìƒíƒœ í™•ì¸

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source .venv/bin/activate

# Phase 1-2 ì™„ë£Œ í™•ì¸
ls -lh ocad/models/tcn/*vv2.0.0.*
# ì˜ˆìƒ ê²°ê³¼: UDP Echo, eCPRI, LBM ëª¨ë¸ íŒŒì¼ 6ê°œ

# ì§„í–‰ ë¦¬í¬íŠ¸ í™•ì¸
cat docs/PROGRESS-REPORT-20251030.md
```

### 2. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸

```bash
# Scikit-learn ì„¤ì¹˜ í™•ì¸
python -c "import sklearn; print(sklearn.__version__)"

# ì—†ë‹¤ë©´ ì„¤ì¹˜
pip install scikit-learn
```

---

## ğŸ¯ Phase 3 ì‘ì—… ë‹¨ê³„

### Step 1: ë‹¤ë³€ëŸ‰ ë°ì´í„° ì¤€ë¹„ (30ë¶„)

**ëª©í‘œ**: 4ê°œ ë©”íŠ¸ë¦­ì„ ë™ì‹œì— ê³ ë ¤í•˜ëŠ” Wide í˜•ì‹ ë°ì´í„° ìƒì„±

#### 1.1 ë‹¤ë³€ëŸ‰ ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

**íŒŒì¼**: `scripts/prepare_multivariate_data.py`

```python
#!/usr/bin/env python3
"""ë‹¤ë³€ëŸ‰ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„."""

import argparse
from pathlib import Path
import pandas as pd
import numpy as np

def prepare_multivariate_data(input_path, output_dir, window_size=10):
    """Wide í˜•ì‹ ë‹¤ë³€ëŸ‰ ë°ì´í„° ìƒì„±.
    
    Args:
        input_path: ì…ë ¥ CSV íŒŒì¼ (ì˜ˆ: 01_normal_operation_24h.csv)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        window_size: ìœˆë„ìš° í¬ê¸° (í†µê³„ ê³„ì‚°ìš©)
    
    Returns:
        ìƒì„±ëœ parquet íŒŒì¼ ê²½ë¡œ
    """
    print(f"ë‹¤ë³€ëŸ‰ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    print(f"  ì…ë ¥: {input_path}")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(input_path)
    print(f"  ì´ ë ˆì½”ë“œ: {len(df):,}")
    
    # í•„ìš”í•œ ë©”íŠ¸ë¦­ ì»¬ëŸ¼
    metric_cols = [
        'udp_echo_rtt_ms',
        'ecpri_delay_us', 
        'lbm_rtt_ms',
        'ccm_miss_count'
    ]
    
    # ë©”íŠ¸ë¦­ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    for col in metric_cols:
        if col not in df.columns:
            raise ValueError(f"ì»¬ëŸ¼ ì—†ìŒ: {col}")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Endpointë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í†µê³„ ê³„ì‚°
    features = []
    
    for endpoint_id, group in df.groupby('endpoint_id'):
        group = group.sort_values('timestamp').reset_index(drop=True)
        
        for i in range(len(group) - window_size + 1):
            window = group.iloc[i:i+window_size]
            
            feature_row = {
                'timestamp': window.iloc[-1]['timestamp'],
                'endpoint_id': endpoint_id,
            }
            
            # ê° ë©”íŠ¸ë¦­ì˜ í†µê³„ëŸ‰ ê³„ì‚°
            for metric in metric_cols:
                values = window[metric].values
                feature_row[f'{metric}_mean'] = np.mean(values)
                feature_row[f'{metric}_std'] = np.std(values)
                feature_row[f'{metric}_min'] = np.min(values)
                feature_row[f'{metric}_max'] = np.max(values)
                feature_row[f'{metric}_last'] = values[-1]
            
            # ì´ìƒ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            feature_row['is_anomaly'] = False
            
            features.append(feature_row)
    
    result_df = pd.DataFrame(features)
    print(f"  ìƒì„±ëœ ìƒ˜í”Œ: {len(result_df):,}")
    
    # Train/Val/Test ë¶„í• 
    result_df = result_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    n = len(result_df)
    train_end = int(n * 0.8)
    val_end = train_end + int(n * 0.1)
    
    train_df = result_df[:train_end]
    val_df = result_df[train_end:val_end]
    test_df = result_df[val_end:]
    
    # ì €ì¥
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    train_path = output_dir / 'multivariate_train.parquet'
    val_path = output_dir / 'multivariate_val.parquet'
    test_path = output_dir / 'multivariate_test.parquet'
    
    train_df.to_parquet(train_path, index=False)
    val_df.to_parquet(val_path, index=False)
    test_df.to_parquet(test_path, index=False)
    
    print(f"\nì €ì¥ ì™„ë£Œ:")
    print(f"  Train: {train_path} ({len(train_df):,} samples)")
    print(f"  Val:   {val_path} ({len(val_df):,} samples)")
    print(f"  Test:  {test_path} ({len(test_df):,} samples)")
    
    return train_path, val_path, test_path


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', default='data/samples/01_normal_operation_24h.csv')
    parser.add_argument('--output-dir', default='data/processed')
    parser.add_argument('--window-size', type=int, default=10)
    
    args = parser.parse_args()
    prepare_multivariate_data(args.input, args.output_dir, args.window_size)
```

#### 1.2 ì‹¤í–‰

```bash
python scripts/prepare_multivariate_data.py \
  --input data/samples/01_normal_operation_24h.csv \
  --output-dir data/processed \
  --window-size 10
```

**ì˜ˆìƒ ê²°ê³¼**:
- `data/processed/multivariate_train.parquet`
- `data/processed/multivariate_val.parquet`
- `data/processed/multivariate_test.parquet`

---

### Step 2: Isolation Forest í•™ìŠµ (30ë¶„)

**ëª©í‘œ**: ë‹¤ë³€ëŸ‰ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ

#### 2.1 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

**íŒŒì¼**: `scripts/train_isolation_forest.py`

```python
#!/usr/bin/env python3
"""Isolation Forest ëª¨ë¸ í•™ìŠµ."""

import argparse
import json
from pathlib import Path
from datetime import datetime
import pickle

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support

def train_isolation_forest(
    train_path,
    val_path,
    test_path,
    output_dir,
    n_estimators=100,
    contamination=0.1,
    random_state=42,
    version='v1.0.0'
):
    """Isolation Forest ëª¨ë¸ í•™ìŠµ."""
    
    print("="*70)
    print("Isolation Forest í•™ìŠµ ì‹œì‘")
    print("="*70)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[1/5] ë°ì´í„° ë¡œë“œ...")
    train_df = pd.read_parquet(train_path)
    val_df = pd.read_parquet(val_path)
    test_df = pd.read_parquet(test_path)
    
    print(f"  Train: {len(train_df):,} samples")
    print(f"  Val:   {len(val_df):,} samples")
    print(f"  Test:  {len(test_df):,} samples")
    
    # 2. í”¼ì²˜ ì¶”ì¶œ
    print("\n[2/5] í”¼ì²˜ ì¶”ì¶œ...")
    feature_cols = [col for col in train_df.columns 
                    if col not in ['timestamp', 'endpoint_id', 'is_anomaly']]
    
    X_train = train_df[feature_cols].values
    X_val = val_df[feature_cols].values
    X_test = test_df[feature_cols].values
    
    y_test = test_df['is_anomaly'].values  # í…ŒìŠ¤íŠ¸ìš©
    
    print(f"  í”¼ì²˜ ê°œìˆ˜: {len(feature_cols)}")
    print(f"  í”¼ì²˜ ëª©ë¡: {feature_cols[:5]}... (ì´ {len(feature_cols)}ê°œ)")
    
    # 3. ì •ê·œí™”
    print("\n[3/5] ë°ì´í„° ì •ê·œí™”...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"  Scaler í•™ìŠµ ì™„ë£Œ")
    print(f"    Mean: {scaler.mean_[:3]}...")
    print(f"    Std:  {scaler.scale_[:3]}...")
    
    # 4. ëª¨ë¸ í•™ìŠµ
    print("\n[4/5] Isolation Forest í•™ìŠµ...")
    model = IsolationForest(
        n_estimators=n_estimators,
        contamination=contamination,
        random_state=random_state,
        n_jobs=-1,
        verbose=1
    )
    
    model.fit(X_train_scaled)
    print(f"  í•™ìŠµ ì™„ë£Œ!")
    
    # 5. í‰ê°€
    print("\n[5/5] ëª¨ë¸ í‰ê°€...")
    
    # Anomaly score ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì´ìƒ)
    train_scores = model.decision_function(X_train_scaled)
    val_scores = model.decision_function(X_val_scaled)
    test_scores = model.decision_function(X_test_scaled)
    
    # Prediction (-1: anomaly, 1: normal)
    test_pred = model.predict(X_test_scaled)
    test_pred_binary = (test_pred == -1).astype(int)
    
    print(f"\ní‰ê°€ ê²°ê³¼:")
    print(f"  Train anomaly score: mean={train_scores.mean():.4f}, std={train_scores.std():.4f}")
    print(f"  Val anomaly score:   mean={val_scores.mean():.4f}, std={val_scores.std():.4f}")
    print(f"  Test anomaly score:  mean={test_scores.mean():.4f}, std={test_scores.std():.4f}")
    print(f"  Test predicted anomalies: {test_pred_binary.sum()} / {len(test_pred_binary)}")
    
    # 6. ëª¨ë¸ ì €ì¥
    print("\n[6/6] ëª¨ë¸ ì €ì¥...")
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = output_dir / f'isolation_forest_{version}.pkl'
    scaler_path = output_dir / f'isolation_forest_{version}_scaler.pkl'
    metadata_path = output_dir / f'isolation_forest_{version}.json'
    
    # ëª¨ë¸ ì €ì¥
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    # Scaler ì €ì¥
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'model_type': 'scikit-learn',
        'algorithm': 'IsolationForest',
        'metadata': {
            'version': version,
            'training_date': datetime.now().isoformat(),
            'n_features': len(feature_cols),
            'feature_names': feature_cols,
        },
        'hyperparameters': {
            'n_estimators': n_estimators,
            'contamination': contamination,
            'random_state': random_state,
        },
        'performance': {
            'train_samples': len(train_df),
            'val_samples': len(val_df),
            'test_samples': len(test_df),
            'train_score_mean': float(train_scores.mean()),
            'train_score_std': float(train_scores.std()),
            'test_score_mean': float(test_scores.mean()),
            'test_score_std': float(test_scores.std()),
            'test_predicted_anomalies': int(test_pred_binary.sum()),
        }
    }
    
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"  ëª¨ë¸: {model_path}")
    print(f"  Scaler: {scaler_path}")
    print(f"  ë©”íƒ€ë°ì´í„°: {metadata_path}")
    
    print("\n" + "="*70)
    print("âœ… Isolation Forest í•™ìŠµ ì™„ë£Œ!")
    print("="*70)
    
    return model_path, metadata_path


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train-data', default='data/processed/multivariate_train.parquet')
    parser.add_argument('--val-data', default='data/processed/multivariate_val.parquet')
    parser.add_argument('--test-data', default='data/processed/multivariate_test.parquet')
    parser.add_argument('--output-dir', default='ocad/models/isolation_forest')
    parser.add_argument('--n-estimators', type=int, default=100)
    parser.add_argument('--contamination', type=float, default=0.1)
    parser.add_argument('--version', default='v1.0.0')
    
    args = parser.parse_args()
    
    train_isolation_forest(
        args.train_data,
        args.val_data,
        args.test_data,
        args.output_dir,
        args.n_estimators,
        args.contamination,
        version=args.version
    )
```

#### 2.2 ì‹¤í–‰

```bash
python scripts/train_isolation_forest.py \
  --train-data data/processed/multivariate_train.parquet \
  --val-data data/processed/multivariate_val.parquet \
  --test-data data/processed/multivariate_test.parquet \
  --output-dir ocad/models/isolation_forest \
  --n-estimators 100 \
  --contamination 0.1 \
  --version v1.0.0
```

**ì˜ˆìƒ ê²°ê³¼**:
- `ocad/models/isolation_forest/isolation_forest_v1.0.0.pkl`
- `ocad/models/isolation_forest/isolation_forest_v1.0.0_scaler.pkl`
- `ocad/models/isolation_forest/isolation_forest_v1.0.0.json`

---

### Step 3: ëª¨ë¸ ê²€ì¦ (20ë¶„)

#### 3.1 ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

**íŒŒì¼**: `scripts/test_isolation_forest.py`

```python
#!/usr/bin/env python3
"""Isolation Forest ëª¨ë¸ ê²€ì¦."""

import pickle
import json
from pathlib import Path
import pandas as pd
import numpy as np

def test_isolation_forest(model_path, scaler_path, metadata_path, test_data_path):
    """Isolation Forest ëª¨ë¸ ê²€ì¦."""
    
    print("="*70)
    print("Isolation Forest ëª¨ë¸ ê²€ì¦")
    print("="*70)
    
    # 1. ë©”íƒ€ë°ì´í„° ë¡œë“œ
    with open(metadata_path) as f:
        metadata = json.load(f)
    
    print(f"\n[1/4] ë©”íƒ€ë°ì´í„°:")
    print(f"  ë²„ì „: {metadata['metadata']['version']}")
    print(f"  í•™ìŠµ ë‚ ì§œ: {metadata['metadata']['training_date']}")
    print(f"  í”¼ì²˜ ê°œìˆ˜: {metadata['metadata']['n_features']}")
    print(f"  N Estimators: {metadata['hyperparameters']['n_estimators']}")
    print(f"  Contamination: {metadata['hyperparameters']['contamination']}")
    
    # 2. ëª¨ë¸ ë¡œë“œ
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)
    
    print(f"\n[2/4] ëª¨ë¸ ë¡œë“œ:")
    print(f"  ëª¨ë¸ íƒ€ì…: {type(model).__name__}")
    print(f"  Scaler íƒ€ì…: {type(scaler).__name__}")
    
    # 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_df = pd.read_parquet(test_data_path)
    
    feature_cols = [col for col in test_df.columns 
                    if col not in ['timestamp', 'endpoint_id', 'is_anomaly']]
    
    X_test = test_df[feature_cols].values
    X_test_scaled = scaler.transform(X_test)
    
    # 4. ì¶”ë¡ 
    scores = model.decision_function(X_test_scaled)
    predictions = model.predict(X_test_scaled)
    
    print(f"\n[3/4] ì¶”ë¡  í…ŒìŠ¤íŠ¸:")
    print(f"  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_df):,}")
    print(f"  Anomaly score ë²”ìœ„: [{scores.min():.4f}, {scores.max():.4f}]")
    print(f"  Anomaly score mean: {scores.mean():.4f}")
    print(f"  ì˜ˆì¸¡ëœ ì´ìƒ ê°œìˆ˜: {(predictions == -1).sum()} / {len(predictions)}")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\n[4/4] ìƒ˜í”Œ ì˜ˆì¸¡:")
    for i in range(min(5, len(test_df))):
        pred_label = "ì´ìƒ" if predictions[i] == -1 else "ì •ìƒ"
        print(f"  ìƒ˜í”Œ {i+1}: score={scores[i]:.4f}, ì˜ˆì¸¡={pred_label}")
    
    print("\n" + "="*70)
    print("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
    print("="*70)


if __name__ == '__main__':
    model_path = Path('ocad/models/isolation_forest/isolation_forest_v1.0.0.pkl')
    scaler_path = Path('ocad/models/isolation_forest/isolation_forest_v1.0.0_scaler.pkl')
    metadata_path = Path('ocad/models/isolation_forest/isolation_forest_v1.0.0.json')
    test_data_path = Path('data/processed/multivariate_test.parquet')
    
    test_isolation_forest(model_path, scaler_path, metadata_path, test_data_path)
```

#### 3.2 ì‹¤í–‰

```bash
python scripts/test_isolation_forest.py
```

---

### Step 4: Phase 3 ì™„ë£Œ í™•ì¸ (10ë¶„)

```bash
# ìƒì„±ëœ íŒŒì¼ í™•ì¸
ls -lh data/processed/multivariate_*.parquet
ls -lh ocad/models/isolation_forest/*

# ì „ì²´ ëª¨ë¸ ëª©ë¡
echo "=== í•™ìŠµëœ ëª¨ë¸ ëª©ë¡ ==="
echo ""
echo "TCN ëª¨ë¸ (3ê°œ):"
ls -lh ocad/models/tcn/*vv2.0.0.pth
echo ""
echo "Isolation Forest ëª¨ë¸ (1ê°œ):"
ls -lh ocad/models/isolation_forest/*.pkl
```

---

## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

Phase 3 ì™„ë£Œ ì‹œ ë‹¤ìŒ í•­ëª©ë“¤ì´ ëª¨ë‘ ì²´í¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:

- [ ] `scripts/prepare_multivariate_data.py` ì‘ì„± ì™„ë£Œ
- [ ] ë‹¤ë³€ëŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ (train/val/test parquet íŒŒì¼)
- [ ] `scripts/train_isolation_forest.py` ì‘ì„± ì™„ë£Œ
- [ ] Isolation Forest ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (.pkl íŒŒì¼)
- [ ] Scaler ì €ì¥ ì™„ë£Œ (_scaler.pkl íŒŒì¼)
- [ ] ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ (.json íŒŒì¼)
- [ ] `scripts/test_isolation_forest.py` ì‘ì„± ì™„ë£Œ
- [ ] ëª¨ë¸ ê²€ì¦ í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„ (Phase 4)

Phase 3 ì™„ë£Œ í›„, Phase 4ì—ì„œëŠ” í•™ìŠµëœ ëª¨ë¸ë“¤ì„ ì‹¤ì œ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì— í†µí•©í•©ë‹ˆë‹¤.

**ì£¼ìš” ì‘ì—…**:
1. `ocad/detectors/residual.py` ìˆ˜ì • - TCN ëª¨ë¸ ë¡œë“œ ê¸°ëŠ¥
2. `ocad/detectors/multivariate.py` ìˆ˜ì • - Isolation Forest ë¡œë“œ ê¸°ëŠ¥
3. `config/local.yaml` ì—…ë°ì´íŠ¸ - ëª¨ë¸ ê²½ë¡œ ì„¤ì •
4. í†µí•© í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ì¸¡ì •

**ì°¸ê³  ë¬¸ì„œ**: 
- [docs/TODO.md](./TODO.md)
- [docs/Training-Inference-Separation-Design.md](./Training-Inference-Separation-Design.md)

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ë©”ëª¨ë¦¬ ë¶€ì¡±
**í•´ê²°**: `n_estimators` ì¤„ì´ê¸° (100 â†’ 50)

### ë¬¸ì œ 2: í•™ìŠµ ì‹œê°„ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
**í•´ê²°**: ë°ì´í„° ìƒ˜í”Œë§ ë˜ëŠ” `n_jobs=-1` ì„¤ì • í™•ì¸

### ë¬¸ì œ 3: Contamination íŒŒë¼ë¯¸í„° ì„ íƒ
**ê¶Œì¥ê°’**: 0.05 ~ 0.15 (ë°ì´í„°ì˜ 5-15%ê°€ ì´ìƒì¹˜ë¼ê³  ê°€ì •)

---

**ì‘ì„±ì¼**: 2025-10-30  
**ì˜ˆì • ì‹¤í–‰ì¼**: 2025-10-31  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1-2ì‹œê°„
