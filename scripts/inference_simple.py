#!/usr/bin/env python3
"""ê°„ë‹¨í•œ OCAD ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸.

í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì´ìƒ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import argparse
import json
from datetime import datetime
from pathlib import Path
import pandas as pd
import pickle
import torch
import sys

# OCAD ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
sys.path.insert(0, str(Path(__file__).parent.parent))

from ocad.detectors.residual import ResidualDetector
from ocad.detectors.multivariate import MultivariateDetector
from ocad.core.config import DetectionConfig
from ocad.core.logging import get_logger
from ocad.core.models import FeatureVector, Capabilities


logger = get_logger(__name__)


def load_input_data(input_path: Path) -> pd.DataFrame:
    """ì…ë ¥ ë°ì´í„° ë¡œë“œ (CSV, Excel, Parquet ì§€ì›)."""
    suffix = input_path.suffix.lower()

    if suffix == '.csv':
        df = pd.read_csv(input_path)
    elif suffix in ['.xlsx', '.xls']:
        df = pd.read_excel(input_path)
    elif suffix == '.parquet':
        df = pd.read_parquet(input_path)
    else:
        raise ValueError(f"Unsupported file format: {suffix}")

    logger.info(f"Loaded {len(df)} records from {input_path}")
    return df


def prepare_features(df: pd.DataFrame) -> tuple[list[FeatureVector], list[datetime]]:
    """ë°ì´í„°í”„ë ˆì„ì„ FeatureVector ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜."""
    features = []
    timestamps = []

    for _, row in df.iterrows():
        # timestamp íŒŒì‹±
        if 'timestamp' in row:
            ts = pd.to_datetime(row['timestamp'])
        else:
            ts = datetime.now()

        timestamps.append(ts)

        # FeatureVector ìƒì„±
        feature = FeatureVector(
            ts_ms=int(ts.timestamp() * 1000),
            endpoint_id=row.get('endpoint_id', 'unknown'),
            window_size_ms=60000,  # 1ë¶„ ìœˆë„ìš°

            # UDP Echo ë©”íŠ¸ë¦­
            udp_echo_p95=float(row.get('udp_echo_rtt_ms', 0)),
            udp_echo_p99=float(row.get('udp_echo_rtt_ms', 0)),
            udp_echo_mean=float(row.get('udp_echo_rtt_ms', 0)),
            udp_echo_ewma=float(row.get('udp_echo_rtt_ms', 0)),
            udp_echo_gradient=0.0,
            udp_echo_cusum=0.0,

            # eCPRI ë©”íŠ¸ë¦­
            ecpri_p95=float(row.get('ecpri_delay_us', 0)),
            ecpri_p99=float(row.get('ecpri_delay_us', 0)),
            ecpri_mean=float(row.get('ecpri_delay_us', 0)),
            ecpri_ewma=float(row.get('ecpri_delay_us', 0)),
            ecpri_gradient=0.0,
            ecpri_cusum=0.0,

            # LBM ë©”íŠ¸ë¦­
            lbm_rtt_p95=float(row.get('lbm_rtt_ms', 0)),
            lbm_rtt_p99=float(row.get('lbm_rtt_ms', 0)),
            lbm_mean=float(row.get('lbm_rtt_ms', 0)),
            lbm_ewma=float(row.get('lbm_rtt_ms', 0)),
            lbm_gradient=0.0,
            lbm_cusum=0.0,

            # CCM ë©”íŠ¸ë¦­
            ccm_miss_count=int(row.get('ccm_miss_count', 0)),
            ccm_miss_rate=float(row.get('ccm_miss_count', 0)),
        )
        features.append(feature)

    return features, timestamps


def run_inference(
    input_path: Path,
    model_dir: Path,
    output_path: Path | None = None,
    use_residual: bool = True,
    use_multivariate: bool = True,
) -> pd.DataFrame:
    """ì¶”ë¡  ì‹¤í–‰."""
    # ì…ë ¥ ë°ì´í„° ë¡œë“œ
    print(f"\n{'='*70}")
    print("OCAD ì¶”ë¡  ì‹¤í–‰")
    print(f"{'='*70}")
    print(f"\nğŸ“‚ ì…ë ¥ ë°ì´í„°: {input_path}")
    print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")

    df = load_input_data(input_path)
    print(f"âœ… {len(df)}ê°œ ë ˆì½”ë“œ ë¡œë“œ ì™„ë£Œ")

    # FeatureVectorë¡œ ë³€í™˜
    print("\nâš™ï¸  í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
    features, timestamps = prepare_features(df)

    # ì„¤ì • ìƒì„±
    config = DetectionConfig(
        use_pretrained_models=True,
        model_path=str(model_dir / "tcn"),
        multivariate_model_path=str(model_dir / "isolation_forest"),
    )

    # Detector ì´ˆê¸°í™”
    detectors = {}
    if use_residual:
        print("ğŸ”§ ResidualDetector ì´ˆê¸°í™” ì¤‘...")
        detectors['residual'] = ResidualDetector(config)

    if use_multivariate:
        print("ğŸ”§ MultivariateDetector ì´ˆê¸°í™” ì¤‘...")
        detectors['multivariate'] = MultivariateDetector(config)

    # ê¸°ë³¸ capabilities (ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”)
    capabilities = Capabilities(
        ccm_min=True,
        lbm=True,
        udp_echo=True,
        ecpri_delay=True,
        lldp=True,
    )

    # ì¶”ë¡  ì‹¤í–‰
    print(f"\nğŸš€ {len(features)}ê°œ ìƒ˜í”Œì— ëŒ€í•œ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    results = []

    for i, (feature, ts) in enumerate(zip(features, timestamps)):
        if i % 100 == 0 and i > 0:
            print(f"   ì§„í–‰: {i}/{len(features)} ({i/len(features)*100:.1f}%)")

        result = {
            'timestamp': ts,
            'endpoint_id': feature.endpoint_id,
        }

        # ê° detector ì‹¤í–‰ (ìƒì„¸ ì •ë³´ í¬í•¨)
        for name, detector in detectors.items():
            try:
                # detect_detailed() ì‚¬ìš©í•˜ì—¬ ìƒì„¸ ì •ë³´ ì–»ê¸°
                detection_result = detector.detect_detailed(feature, capabilities)

                # ê¸°ë³¸ ì ìˆ˜
                result[f'{name}_score'] = detection_result.score
                result[f'{name}_anomaly'] = 1 if detection_result.is_anomaly else 0

                # ìƒì„¸ ì •ë³´ ì¶”ê°€
                if name == 'residual':
                    # ResidualDetector: ê° ë©”íŠ¸ë¦­ì˜ ì˜ˆì¸¡ê°’, ì‹¤ì œê°’, ì˜¤ì°¨
                    for metric_name, detail in detection_result.metric_details.items():
                        prefix = f'residual_{metric_name}'
                        result[f'{prefix}_actual'] = detail.actual_value
                        result[f'{prefix}_predicted'] = detail.predicted_value
                        result[f'{prefix}_error'] = detail.error
                        result[f'{prefix}_normalized_error'] = detail.normalized_error

                    result['residual_explanation'] = detection_result.explanation

                elif name == 'multivariate':
                    # MultivariateDetector: ë‹¤ë³€ëŸ‰ íŒ¨í„´ ì •ë³´
                    result['multivariate_explanation'] = detection_result.explanation
                    if detection_result.dominant_metric:
                        result['multivariate_dominant_metric'] = detection_result.dominant_metric

            except Exception as e:
                logger.error(f"Error in {name} detector: {e}")
                result[f'{name}_score'] = 0.0
                result[f'{name}_anomaly'] = 0

        # ìµœì¢… ì´ìƒ ì—¬ë¶€ (OR ì¡°í•©)
        anomaly_scores = [result.get(f'{name}_score', 0) for name in detectors.keys()]
        result['final_score'] = max(anomaly_scores) if anomaly_scores else 0.0
        result['is_anomaly'] = 1 if result['final_score'] > 0.5 else 0

        results.append(result)

    # ê²°ê³¼ DataFrame ìƒì„±
    results_df = pd.DataFrame(results)

    # í†µê³„ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“Š ì¶”ë¡  ê²°ê³¼ ìš”ì•½")
    print("="*70)
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(results_df)}")
    print(f"ì´ìƒ íƒì§€ ìˆ˜: {results_df['is_anomaly'].sum()}")
    print(f"ì´ìƒ íƒì§€ìœ¨: {results_df['is_anomaly'].mean():.2%}")

    for name in detectors.keys():
        score_col = f'{name}_score'
        if score_col in results_df.columns:
            print(f"\n{name.upper()} Detector:")
            print(f"  í‰ê·  ì ìˆ˜: {results_df[score_col].mean():.4f}")
            print(f"  ìµœëŒ€ ì ìˆ˜: {results_df[score_col].max():.4f}")
            print(f"  ì´ìƒ ìˆ˜: {results_df[f'{name}_anomaly'].sum()}")

    print("="*70)

    # ê²°ê³¼ ì €ì¥
    if output_path:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        suffix = output_path.suffix.lower()
        if suffix == '.csv':
            results_df.to_csv(output_path, index=False)
        elif suffix == '.parquet':
            results_df.to_parquet(output_path, index=False)
        else:
            results_df.to_csv(output_path.with_suffix('.csv'), index=False)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    return results_df


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(
        description="OCAD ì¶”ë¡  ì‹¤í–‰ (ê°„ë‹¨ ë²„ì „)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # í•„ìˆ˜ ì¸ì
    parser.add_argument(
        "--input",
        type=Path,
        required=True,
        help="ì…ë ¥ ë°ì´í„° íŒŒì¼ (CSV, Excel, Parquet)",
    )

    # ì„ íƒì  ì¸ì
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ ìë™ ìƒì„±)",
    )
    parser.add_argument(
        "--model-dir",
        type=Path,
        default=Path("ocad/models"),
        help="í•™ìŠµëœ ëª¨ë¸ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--no-residual",
        action="store_true",
        help="ResidualDetector ë¹„í™œì„±í™”",
    )
    parser.add_argument(
        "--no-multivariate",
        action="store_true",
        help="MultivariateDetector ë¹„í™œì„±í™”",
    )

    args = parser.parse_args()

    # ì¶œë ¥ ê²½ë¡œ ìë™ ìƒì„±
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output = Path(f"data/results/inference_{timestamp}.csv")

    # ì¶”ë¡  ì‹¤í–‰
    run_inference(
        input_path=args.input,
        model_dir=args.model_dir,
        output_path=args.output,
        use_residual=not args.no_residual,
        use_multivariate=not args.no_multivariate,
    )


if __name__ == "__main__":
    main()
